{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Fast Whisper Tokenizer for Tibetan\n",
    "\n",
    "This notebook demonstrates how to train a Whisper tokenizer for Tibetan text using the `PreTrainedTokenizerFast` class, which correctly supports the `train_new_from_iterator` method.\n",
    "\n",
    "We'll follow these steps:\n",
    "1. Load a fast tokenizer from the Whisper model\n",
    "2. Prepare a Tibetan text corpus\n",
    "3. Train the tokenizer on the Tibetan corpus\n",
    "4. Test and evaluate the tokenizer\n",
    "5. Save the trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install transformers tokenizers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from transformers import PreTrainedTokenizerFast, WhisperProcessor\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Base Fast Tokenizer\n",
    "\n",
    "We'll load the tokenizer from the Whisper model using `PreTrainedTokenizerFast` which supports the `train_new_from_iterator` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading fast tokenizer from /home/gangagyatso/Desktop/stt-bpe-trainer/data/whisper_tokenizer_added_tibetan...\n",
      "‚úÖ Fast tokenizer loaded successfully\n",
      "Vocabulary size: 53014\n"
     ]
    }
   ],
   "source": [
    "# Define the base model\n",
    "BASE_MODEL = \"/home/gangagyatso/Desktop/stt-bpe-trainer/data/whisper_tokenizer_added_tibetan\"\n",
    "\n",
    "# Load the fast tokenizer\n",
    "print(f\"üöÄ Loading fast tokenizer from {BASE_MODEL}...\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(BASE_MODEL)\n",
    "print(\"‚úÖ Fast tokenizer loaded successfully\")\n",
    "\n",
    "# Check vocabulary size\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the Tibetan Corpus\n",
    "\n",
    "Create an iterator for the Tibetan corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found corpus file: data/corpus/mergedcorpus.txt\n",
      "\n",
      "Sample of corpus:\n",
      "Line 1: ‡Ω®‡Ω∫‡ºç ‡Ω¶‡Ω¥‡Ω†‡Ω≤‡ºã‡Ω¶‡æ£‡ΩÑ‡ºã‡Ωñ‡Ω¢‡ºã‡Ωë‡Ω∫‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ωì‡ºã\n",
      "Line 2: ‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡ΩÖ‡Ωì‡ºã‡Ωê‡Ωò‡Ω¶‡ºã‡ΩÖ‡Ωë‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ωö‡Ωñ‡ºã‡Ω£‡ºã‡ΩÇ‡Ωû‡Ωì‡ºã‡Ωñ‡Ω¶‡Ωò‡ºã‡Ωö‡Ω¢‡ºã‡Ωë‡Ω¥‡Ω¶‡ºã‡Ωñ‡Ωë‡ΩÇ‡ºã‡Ω†‡Ωõ‡Ω≤‡Ωì‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ωò‡ºã‡Ω¢‡Ω∫‡Ωë‡ºã\n",
      "Line 3: ‡Ω¢‡ΩÑ‡ºã‡Ωê‡Ωº‡ΩÇ‡ºã‡Ω£‡ºã‡Ωë‡ΩÑ‡Ωº‡Ω¶‡ºã‡Ω¶‡Ω¥‡ºã‡Ω¶‡æô‡Ω≤‡ΩÑ‡ºã‡Ω¢‡æó‡Ω∫‡ºã‡Ω°‡Ω≤‡Ω¶‡ºã‡Ω†‡Ωñ‡æ≤‡Ω∫‡Ω£‡ºã‡Ωñ‡ºã‡Ω°‡Ωº‡ΩÑ‡ºã‡ΩÇ‡Ω≤‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºã\n"
     ]
    }
   ],
   "source": [
    "# Path to your Tibetan corpus file\n",
    "CORPUS_FILE = \"data/corpus/mergedcorpus.txt\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(CORPUS_FILE):\n",
    "    print(f\"‚ùå Corpus file not found: {CORPUS_FILE}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found corpus file: {CORPUS_FILE}\")\n",
    "    \n",
    "    # Look at a sample of the corpus\n",
    "    with open(CORPUS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        sample_lines = [next(f).strip() for _ in range(3) if f]\n",
    "    \n",
    "    print(\"\\nSample of corpus:\")\n",
    "    for i, line in enumerate(sample_lines):\n",
    "        print(f\"Line {i+1}: {line[:100]}...\" if len(line) > 100 else f\"Line {i+1}: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define corpus iterator function\n",
    "def corpus_iterator():\n",
    "    with open(CORPUS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                yield line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Initial Tokenizer on Tibetan\n",
    "\n",
    "Let's first see how the original tokenizer handles Tibetan text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokenizer results:\n",
      "Text: ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω¢‡æ£‡Ωò‡ºã‡Ωî‡ºã‡ΩÇ‡Ωâ‡Ω≤‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç\n",
      "Token count: 18\n",
      "Tokens: ['‡Ωñ‡æ±‡ΩÑ', '‡ºã', '‡ΩÜ‡Ω¥‡Ωñ', '‡ºã', '‡ΩÄ‡æ±‡Ω≤', '‡ºã', '‡Ω¶‡Ω∫‡Ωò‡Ω¶', '‡ºã', '‡Ω¢‡æ£‡Ωò', '‡ºã', '‡Ωî', '‡ºã', '‡ΩÇ‡Ωâ‡Ω≤‡Ω¶', '‡ºã', '‡Ω°‡Ωº‡Ωë', '‡ºã', '‡Ω¢‡Ω∫‡Ωë', '‡ºç']\n",
      "Token IDs: [50258, 50363, 52134, 51866, 52161, 51866, 51999, 51866, 52010, 51866, 52110, 51866, 51893, 51866, 52089, 51866, 51979, 51866, 51971, 51868, 50257]\n",
      "\n",
      "Roundtrip: True\n",
      "Decoded text: ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω¢‡æ£‡Ωò‡ºã‡Ωî‡ºã‡ΩÇ‡Ωâ‡Ω≤‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Example Tibetan text\n",
    "tibetan_example = \"‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω¢‡æ£‡Ωò‡ºã‡Ωî‡ºã‡ΩÇ‡Ωâ‡Ω≤‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç\"\n",
    "\n",
    "# Tokenize with the original tokenizer\n",
    "tokens = tokenizer.tokenize(tibetan_example)\n",
    "token_ids = tokenizer.encode(tibetan_example)\n",
    "\n",
    "print(\"Original tokenizer results:\")\n",
    "print(f\"Text: {tibetan_example}\")\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Check roundtrip\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"\\nRoundtrip: {tibetan_example in decoded}\")\n",
    "print(f\"Decoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Tokenizer on Tibetan Corpus\n",
    "\n",
    "Now we'll train the tokenizer on our Tibetan corpus to improve its handling of Tibetan text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Original vocabulary size: 53014\n",
      "üéØ Target vocabulary size: 63014\n",
      "‚è≥ Starting tokenizer training... This may take a few minutes.\n",
      "The OrderedVocab you are attempting to save contains holes for indices [50258, 50259, 50260, 50261, 50262, 50263, 50264, 50265, 50266, 50267, 50268, 50269, 50270, 50271, 50272, 50273, 50274, 50275, 50276, 50277, 50278, 50279, 50280, 50281, 50282, 50283, 50284, 50285, 50286, 50287, 50288, 50289, 50290, 50291, 50292, 50293, 50294, 50295, 50296, 50297, 50298, 50299, 50300, 50301, 50302, 50303, 50304, 50305, 50306, 50307, 50308, 50309, 50310, 50311, 50312, 50313, 50314, 50315, 50316, 50317, 50318, 50319, 50320, 50321, 50322, 50323, 50324, 50325, 50326, 50327, 50328, 50329, 50330, 50331, 50332, 50333, 50334, 50335, 50336, 50337, 50338, 50339, 50340, 50341, 50342, 50343, 50344, 50345, 50346, 50347, 50348, 50349, 50350, 50351, 50352, 50353, 50354, 50355, 50356, 50357, 50358, 50359, 50360, 50361, 50362, 50363, 50364, 50365, 50366, 50367, 50368, 50369, 50370, 50371, 50372, 50373, 50374, 50375, 50376, 50377, 50378, 50379, 50380, 50381, 50382, 50383, 50384, 50385, 50386, 50387, 50388, 50389, 50390, 50391, 50392, 50393, 50394, 50395, 50396, 50397, 50398, 50399, 50400, 50401, 50402, 50403, 50404, 50405, 50406, 50407, 50408, 50409, 50410, 50411, 50412, 50413, 50414, 50415, 50416, 50417, 50418, 50419, 50420, 50421, 50422, 50423, 50424, 50425, 50426, 50427, 50428, 50429, 50430, 50431, 50432, 50433, 50434, 50435, 50436, 50437, 50438, 50439, 50440, 50441, 50442, 50443, 50444, 50445, 50446, 50447, 50448, 50449, 50450, 50451, 50452, 50453, 50454, 50455, 50456, 50457, 50458, 50459, 50460, 50461, 50462, 50463, 50464, 50465, 50466, 50467, 50468, 50469, 50470, 50471, 50472, 50473, 50474, 50475, 50476, 50477, 50478, 50479, 50480, 50481, 50482, 50483, 50484, 50485, 50486, 50487, 50488, 50489, 50490, 50491, 50492, 50493, 50494, 50495, 50496, 50497, 50498, 50499, 50500, 50501, 50502, 50503, 50504, 50505, 50506, 50507, 50508, 50509, 50510, 50511, 50512, 50513, 50514, 50515, 50516, 50517, 50518, 50519, 50520, 50521, 50522, 50523, 50524, 50525, 50526, 50527, 50528, 50529, 50530, 50531, 50532, 50533, 50534, 50535, 50536, 50537, 50538, 50539, 50540, 50541, 50542, 50543, 50544, 50545, 50546, 50547, 50548, 50549, 50550, 50551, 50552, 50553, 50554, 50555, 50556, 50557, 50558, 50559, 50560, 50561, 50562, 50563, 50564, 50565, 50566, 50567, 50568, 50569, 50570, 50571, 50572, 50573, 50574, 50575, 50576, 50577, 50578, 50579, 50580, 50581, 50582, 50583, 50584, 50585, 50586, 50587, 50588, 50589, 50590, 50591, 50592, 50593, 50594, 50595, 50596, 50597, 50598, 50599, 50600, 50601, 50602, 50603, 50604, 50605, 50606, 50607, 50608, 50609, 50610, 50611, 50612, 50613, 50614, 50615, 50616, 50617, 50618, 50619, 50620, 50621, 50622, 50623, 50624, 50625, 50626, 50627, 50628, 50629, 50630, 50631, 50632, 50633, 50634, 50635, 50636, 50637, 50638, 50639, 50640, 50641, 50642, 50643, 50644, 50645, 50646, 50647, 50648, 50649, 50650, 50651, 50652, 50653, 50654, 50655, 50656, 50657, 50658, 50659, 50660, 50661, 50662, 50663, 50664, 50665, 50666, 50667, 50668, 50669, 50670, 50671, 50672, 50673, 50674, 50675, 50676, 50677, 50678, 50679, 50680, 50681, 50682, 50683, 50684, 50685, 50686, 50687, 50688, 50689, 50690, 50691, 50692, 50693, 50694, 50695, 50696, 50697, 50698, 50699, 50700, 50701, 50702, 50703, 50704, 50705, 50706, 50707, 50708, 50709, 50710, 50711, 50712, 50713, 50714, 50715, 50716, 50717, 50718, 50719, 50720, 50721, 50722, 50723, 50724, 50725, 50726, 50727, 50728, 50729, 50730, 50731, 50732, 50733, 50734, 50735, 50736, 50737, 50738, 50739, 50740, 50741, 50742, 50743, 50744, 50745, 50746, 50747, 50748, 50749, 50750, 50751, 50752, 50753, 50754, 50755, 50756, 50757, 50758, 50759, 50760, 50761, 50762, 50763, 50764, 50765, 50766, 50767, 50768, 50769, 50770, 50771, 50772, 50773, 50774, 50775, 50776, 50777, 50778, 50779, 50780, 50781, 50782, 50783, 50784, 50785, 50786, 50787, 50788, 50789, 50790, 50791, 50792, 50793, 50794, 50795, 50796, 50797, 50798, 50799, 50800, 50801, 50802, 50803, 50804, 50805, 50806, 50807, 50808, 50809, 50810, 50811, 50812, 50813, 50814, 50815, 50816, 50817, 50818, 50819, 50820, 50821, 50822, 50823, 50824, 50825, 50826, 50827, 50828, 50829, 50830, 50831, 50832, 50833, 50834, 50835, 50836, 50837, 50838, 50839, 50840, 50841, 50842, 50843, 50844, 50845, 50846, 50847, 50848, 50849, 50850, 50851, 50852, 50853, 50854, 50855, 50856, 50857, 50858, 50859, 50860, 50861, 50862, 50863, 50864, 50865, 50866, 50867, 50868, 50869, 50870, 50871, 50872, 50873, 50874, 50875, 50876, 50877, 50878, 50879, 50880, 50881, 50882, 50883, 50884, 50885, 50886, 50887, 50888, 50889, 50890, 50891, 50892, 50893, 50894, 50895, 50896, 50897, 50898, 50899, 50900, 50901, 50902, 50903, 50904, 50905, 50906, 50907, 50908, 50909, 50910, 50911, 50912, 50913, 50914, 50915, 50916, 50917, 50918, 50919, 50920, 50921, 50922, 50923, 50924, 50925, 50926, 50927, 50928, 50929, 50930, 50931, 50932, 50933, 50934, 50935, 50936, 50937, 50938, 50939, 50940, 50941, 50942, 50943, 50944, 50945, 50946, 50947, 50948, 50949, 50950, 50951, 50952, 50953, 50954, 50955, 50956, 50957, 50958, 50959, 50960, 50961, 50962, 50963, 50964, 50965, 50966, 50967, 50968, 50969, 50970, 50971, 50972, 50973, 50974, 50975, 50976, 50977, 50978, 50979, 50980, 50981, 50982, 50983, 50984, 50985, 50986, 50987, 50988, 50989, 50990, 50991, 50992, 50993, 50994, 50995, 50996, 50997, 50998, 50999, 51000, 51001, 51002, 51003, 51004, 51005, 51006, 51007, 51008, 51009, 51010, 51011, 51012, 51013, 51014, 51015, 51016, 51017, 51018, 51019, 51020, 51021, 51022, 51023, 51024, 51025, 51026, 51027, 51028, 51029, 51030, 51031, 51032, 51033, 51034, 51035, 51036, 51037, 51038, 51039, 51040, 51041, 51042, 51043, 51044, 51045, 51046, 51047, 51048, 51049, 51050, 51051, 51052, 51053, 51054, 51055, 51056, 51057, 51058, 51059, 51060, 51061, 51062, 51063, 51064, 51065, 51066, 51067, 51068, 51069, 51070, 51071, 51072, 51073, 51074, 51075, 51076, 51077, 51078, 51079, 51080, 51081, 51082, 51083, 51084, 51085, 51086, 51087, 51088, 51089, 51090, 51091, 51092, 51093, 51094, 51095, 51096, 51097, 51098, 51099, 51100, 51101, 51102, 51103, 51104, 51105, 51106, 51107, 51108, 51109, 51110, 51111, 51112, 51113, 51114, 51115, 51116, 51117, 51118, 51119, 51120, 51121, 51122, 51123, 51124, 51125, 51126, 51127, 51128, 51129, 51130, 51131, 51132, 51133, 51134, 51135, 51136, 51137, 51138, 51139, 51140, 51141, 51142, 51143, 51144, 51145, 51146, 51147, 51148, 51149, 51150, 51151, 51152, 51153, 51154, 51155, 51156, 51157, 51158, 51159, 51160, 51161, 51162, 51163, 51164, 51165, 51166, 51167, 51168, 51169, 51170, 51171, 51172, 51173, 51174, 51175, 51176, 51177, 51178, 51179, 51180, 51181, 51182, 51183, 51184, 51185, 51186, 51187, 51188, 51189, 51190, 51191, 51192, 51193, 51194, 51195, 51196, 51197, 51198, 51199, 51200, 51201, 51202, 51203, 51204, 51205, 51206, 51207, 51208, 51209, 51210, 51211, 51212, 51213, 51214, 51215, 51216, 51217, 51218, 51219, 51220, 51221, 51222, 51223, 51224, 51225, 51226, 51227, 51228, 51229, 51230, 51231, 51232, 51233, 51234, 51235, 51236, 51237, 51238, 51239, 51240, 51241, 51242, 51243, 51244, 51245, 51246, 51247, 51248, 51249, 51250, 51251, 51252, 51253, 51254, 51255, 51256, 51257, 51258, 51259, 51260, 51261, 51262, 51263, 51264, 51265, 51266, 51267, 51268, 51269, 51270, 51271, 51272, 51273, 51274, 51275, 51276, 51277, 51278, 51279, 51280, 51281, 51282, 51283, 51284, 51285, 51286, 51287, 51288, 51289, 51290, 51291, 51292, 51293, 51294, 51295, 51296, 51297, 51298, 51299, 51300, 51301, 51302, 51303, 51304, 51305, 51306, 51307, 51308, 51309, 51310, 51311, 51312, 51313, 51314, 51315, 51316, 51317, 51318, 51319, 51320, 51321, 51322, 51323, 51324, 51325, 51326, 51327, 51328, 51329, 51330, 51331, 51332, 51333, 51334, 51335, 51336, 51337, 51338, 51339, 51340, 51341, 51342, 51343, 51344, 51345, 51346, 51347, 51348, 51349, 51350, 51351, 51352, 51353, 51354, 51355, 51356, 51357, 51358, 51359, 51360, 51361, 51362, 51363, 51364, 51365, 51366, 51367, 51368, 51369, 51370, 51371, 51372, 51373, 51374, 51375, 51376, 51377, 51378, 51379, 51380, 51381, 51382, 51383, 51384, 51385, 51386, 51387, 51388, 51389, 51390, 51391, 51392, 51393, 51394, 51395, 51396, 51397, 51398, 51399, 51400, 51401, 51402, 51403, 51404, 51405, 51406, 51407, 51408, 51409, 51410, 51411, 51412, 51413, 51414, 51415, 51416, 51417, 51418, 51419, 51420, 51421, 51422, 51423, 51424, 51425, 51426, 51427, 51428, 51429, 51430, 51431, 51432, 51433, 51434, 51435, 51436, 51437, 51438, 51439, 51440, 51441, 51442, 51443, 51444, 51445, 51446, 51447, 51448, 51449, 51450, 51451, 51452, 51453, 51454, 51455, 51456, 51457, 51458, 51459, 51460, 51461, 51462, 51463, 51464, 51465, 51466, 51467, 51468, 51469, 51470, 51471, 51472, 51473, 51474, 51475, 51476, 51477, 51478, 51479, 51480, 51481, 51482, 51483, 51484, 51485, 51486, 51487, 51488, 51489, 51490, 51491, 51492, 51493, 51494, 51495, 51496, 51497, 51498, 51499, 51500, 51501, 51502, 51503, 51504, 51505, 51506, 51507, 51508, 51509, 51510, 51511, 51512, 51513, 51514, 51515, 51516, 51517, 51518, 51519, 51520, 51521, 51522, 51523, 51524, 51525, 51526, 51527, 51528, 51529, 51530, 51531, 51532, 51533, 51534, 51535, 51536, 51537, 51538, 51539, 51540, 51541, 51542, 51543, 51544, 51545, 51546, 51547, 51548, 51549, 51550, 51551, 51552, 51553, 51554, 51555, 51556, 51557, 51558, 51559, 51560, 51561, 51562, 51563, 51564, 51565, 51566, 51567, 51568, 51569, 51570, 51571, 51572, 51573, 51574, 51575, 51576, 51577, 51578, 51579, 51580, 51581, 51582, 51583, 51584, 51585, 51586, 51587, 51588, 51589, 51590, 51591, 51592, 51593, 51594, 51595, 51596, 51597, 51598, 51599, 51600, 51601, 51602, 51603, 51604, 51605, 51606, 51607, 51608, 51609, 51610, 51611, 51612, 51613, 51614, 51615, 51616, 51617, 51618, 51619, 51620, 51621, 51622, 51623, 51624, 51625, 51626, 51627, 51628, 51629, 51630, 51631, 51632, 51633, 51634, 51635, 51636, 51637, 51638, 51639, 51640, 51641, 51642, 51643, 51644, 51645, 51646, 51647, 51648, 51649, 51650, 51651, 51652, 51653, 51654, 51655, 51656, 51657, 51658, 51659, 51660, 51661, 51662, 51663, 51664, 51665, 51666, 51667, 51668, 51669, 51670, 51671, 51672, 51673, 51674, 51675, 51676, 51677, 51678, 51679, 51680, 51681, 51682, 51683, 51684, 51685, 51686, 51687, 51688, 51689, 51690, 51691, 51692, 51693, 51694, 51695, 51696, 51697, 51698, 51699, 51700, 51701, 51702, 51703, 51704, 51705, 51706, 51707, 51708, 51709, 51710, 51711, 51712, 51713, 51714, 51715, 51716, 51717, 51718, 51719, 51720, 51721, 51722, 51723, 51724, 51725, 51726, 51727, 51728, 51729, 51730, 51731, 51732, 51733, 51734, 51735, 51736, 51737, 51738, 51739, 51740, 51741, 51742, 51743, 51744, 51745, 51746, 51747, 51748, 51749, 51750, 51751, 51752, 51753, 51754, 51755, 51756, 51757, 51758, 51759, 51760, 51761, 51762, 51763, 51764, 51765, 51766, 51767, 51768, 51769, 51770, 51771, 51772, 51773, 51774, 51775, 51776, 51777, 51778, 51779, 51780, 51781, 51782, 51783, 51784, 51785, 51786, 51787, 51788, 51789, 51790, 51791, 51792, 51793, 51794, 51795, 51796, 51797, 51798, 51799, 51800, 51801, 51802, 51803, 51804, 51805, 51806, 51807, 51808, 51809, 51810, 51811, 51812, 51813, 51814, 51815, 51816, 51817, 51818, 51819, 51820, 51821, 51822, 51823, 51824, 51825, 51826, 51827, 51828, 51829, 51830, 51831, 51832, 51833, 51834, 51835, 51836, 51837, 51838, 51839, 51840, 51841, 51842, 51843, 51844, 51845, 51846, 51847, 51848, 51849, 51850, 51851, 51852, 51853, 51854, 51855, 51856, 51857, 51858, 51859, 51860, 51861, 51862, 51863, 51864], your vocabulary could be corrupted !\n",
      "\n",
      "\n",
      "\n",
      "‚úÖ Training complete!\n",
      "üìà New vocabulary size: 3059\n"
     ]
    }
   ],
   "source": [
    "# Set target vocabulary size\n",
    "TARGET_VOCAB_SIZE = len(tokenizer) + 10000\n",
    "\n",
    "print(f\"üß† Original vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"üéØ Target vocabulary size: {TARGET_VOCAB_SIZE}\")\n",
    "\n",
    "print(\"‚è≥ Starting tokenizer training... This may take a few minutes.\")\n",
    "\n",
    "# Train the tokenizer\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(corpus_iterator(), vocab_size=TARGET_VOCAB_SIZE, )\n",
    "\n",
    "print(\"‚úÖ Training complete!\")\n",
    "print(f\"üìà New vocabulary size: {len(new_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Trained Tokenizer\n",
    "\n",
    "Let's check how the newly trained tokenizer handles Tibetan text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test examples\n",
    "test_examples = [\n",
    "    \"‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω¢‡æ£‡Ωò‡ºã‡Ωî‡ºã‡ΩÇ‡Ωâ‡Ω≤‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç\",  # Simple sentence\n",
    "    \"‡Ωñ‡Ωº‡Ωë‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ωì‡Ω≤‡ºã‡Ω£‡Ωº‡ºã‡Ω¢‡æí‡æ±‡Ω¥‡Ω¶‡ºã‡Ωß‡ºã‡ΩÖ‡ΩÑ‡ºã‡Ω¢‡Ω≤‡ΩÑ‡ºã‡Ωî‡Ωº‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ωî‡Ω†‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡ΩÖ‡Ω≤‡ΩÇ‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç\",  # Another example\n",
    "    \"‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω¢‡æ£‡Ωò‡ºã‡Ωî‡ºã‡ΩÇ‡Ωâ‡Ω≤‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ωì‡ºã‡Ω°‡Ω∫‡ºã‡Ω§‡Ω∫‡Ω¶‡ºã‡Ω£‡æ∑‡ºã‡Ω£‡ºã‡Ω†‡ΩÇ‡æ±‡Ω¥‡Ω¢‡ºã‡Ω†‡ΩÇ‡æ≤‡Ωº‡ºã‡ΩÇ‡Ω≤‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ωò‡Ω∫‡Ωë‡ºã‡Ωì‡ºç ‡Ωë‡Ω∫‡ºã‡Ωì‡Ω¶‡ºã‡Ω†‡Ωë‡Ω≤‡ºã\"\n",
    "]\n",
    "\n",
    "# Function to analyze tokenization\n",
    "def analyze_tokenization(tokenizer, text, name=\"Tokenizer\"):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    decoded = tokenizer.decode(token_ids)\n",
    "    \n",
    "    print(f\"\\n--- {name} Results ---\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Token count: {len(tokens)}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Roundtrip successful: {text in decoded}\")\n",
    "    \n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Test Example 1 ====\n",
      "\n",
      "\n",
      "--- Original Tokenizer Results ---\n",
      "Text: ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω¢‡æ£‡Ωò‡ºã‡Ωî‡ºã‡ΩÇ‡Ωâ‡Ω≤‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç\n",
      "Token count: 18\n",
      "Tokens: ['‡Ωñ‡æ±‡ΩÑ', '‡ºã', '‡ΩÜ‡Ω¥‡Ωñ', '‡ºã', '‡ΩÄ‡æ±‡Ω≤', '‡ºã', '‡Ω¶‡Ω∫‡Ωò‡Ω¶', '‡ºã', '‡Ω¢‡æ£‡Ωò', '‡ºã', '‡Ωî', '‡ºã', '‡ΩÇ‡Ωâ‡Ω≤‡Ω¶', '‡ºã', '‡Ω°‡Ωº‡Ωë', '‡ºã', '‡Ω¢‡Ω∫‡Ωë', '‡ºç']\n",
      "Roundtrip successful: True\n",
      "\n",
      "--- Trained Tokenizer Results ---\n",
      "Text: ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω¢‡æ£‡Ωò‡ºã‡Ωî‡ºã‡ΩÇ‡Ωâ‡Ω≤‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç\n",
      "Token count: 32\n",
      "Tokens: ['√†¬Ωƒ∏', '√†¬æ¬±', '√†¬Ωƒ¶', '√†¬ºƒ≠', '√†¬Ωƒ®', '√†¬Ω¬¥', '√†¬Ωƒ∏', '√†¬ºƒ≠', '√†¬Ωƒ¢', '√†¬æ¬±√†¬Ω¬≤√†¬ºƒ≠', '√†¬Ω¬¶', '√†¬Ω¬∫', '√†¬Ωƒ∫√†¬Ω¬¶', '√†¬ºƒ≠', '√†¬Ω¬¢', '√†¬æ¬£', '√†¬Ωƒ∫', '√†¬ºƒ≠', '√†¬Ωƒ∂', '√†¬ºƒ≠', '√†¬Ωƒ§√†¬Ωƒ´', '√†¬Ω¬≤', '√†¬Ω¬¶', '√†¬ºƒ≠', '√†¬Ω¬°', '√†¬Ω¬º', '√†¬Ωƒ≥', '√†¬ºƒ≠', '√†¬Ω¬¢', '√†¬Ω¬∫', '√†¬Ωƒ≥', '√†¬ºƒØ']\n",
      "Roundtrip successful: True\n",
      "\n",
      "‚ö†Ô∏è New tokenizer uses 14 more tokens.\n",
      "\n",
      "==== Test Example 2 ====\n",
      "\n",
      "\n",
      "--- Original Tokenizer Results ---\n",
      "Text: ‡Ωñ‡Ωº‡Ωë‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ωì‡Ω≤‡ºã‡Ω£‡Ωº‡ºã‡Ω¢‡æí‡æ±‡Ω¥‡Ω¶‡ºã‡Ωß‡ºã‡ΩÖ‡ΩÑ‡ºã‡Ω¢‡Ω≤‡ΩÑ‡ºã‡Ωî‡Ωº‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ωî‡Ω†‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡ΩÖ‡Ω≤‡ΩÇ‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç\n",
      "Token count: 34\n",
      "Tokens: ['‡Ωñ‡Ωº‡Ωë', '‡ºã', '‡ΩÄ‡æ±‡Ω≤', '‡ºã', '‡Ω¶‡æê‡Ωë', '‡ºã', '‡Ω°‡Ω≤‡ΩÇ', '‡ºã', '‡Ωì‡Ω≤', '‡ºã', '‡Ω£‡Ωº', '‡ºã', '‡Ω¢‡æí‡æ±‡Ω¥‡Ω¶', '‡ºã', '‡Ωß', '‡ºã', '‡ΩÖ‡ΩÑ', '‡ºã', '‡Ω¢‡Ω≤‡ΩÑ', '‡ºã', '‡Ωî‡Ωº', '‡ºã', '‡Ω°‡Ωº‡Ωë', '‡ºã', '‡Ωî‡Ω†‡Ω≤', '‡ºã', '‡Ω¶‡æê‡Ωë', '‡ºã', '‡Ω°‡Ω≤‡ΩÇ', '‡ºã', '‡ΩÖ‡Ω≤‡ΩÇ', '‡ºã', '‡Ω¢‡Ω∫‡Ωë', '‡ºç']\n",
      "Roundtrip successful: True\n",
      "\n",
      "--- Trained Tokenizer Results ---\n",
      "Text: ‡Ωñ‡Ωº‡Ωë‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡Ωì‡Ω≤‡ºã‡Ω£‡Ωº‡ºã‡Ω¢‡æí‡æ±‡Ω¥‡Ω¶‡ºã‡Ωß‡ºã‡ΩÖ‡ΩÑ‡ºã‡Ω¢‡Ω≤‡ΩÑ‡ºã‡Ωî‡Ωº‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ωî‡Ω†‡Ω≤‡ºã‡Ω¶‡æê‡Ωë‡ºã‡Ω°‡Ω≤‡ΩÇ‡ºã‡ΩÖ‡Ω≤‡ΩÇ‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç\n",
      "Token count: 54\n",
      "Tokens: ['√†¬Ωƒ∏', '√†¬Ω¬º', '√†¬Ωƒ≥', '√†¬ºƒ≠', '√†¬Ωƒ¢', '√†¬æ¬±√†¬Ω¬≤√†¬ºƒ≠', '√†¬Ω¬¶', '√†¬æƒ≤', '√†¬Ωƒ≥', '√†¬ºƒ≠', '√†¬Ω¬°', '√†¬Ω¬≤', '√†¬Ωƒ§', '√†¬ºƒ≠', '√†¬Ωƒµ', '√†¬Ω¬≤√†¬ºƒ≠', '√†¬Ω¬£', '√†¬Ω¬º√†¬ºƒ≠', '√†¬Ω¬¢', '√†¬æƒ¥√†¬æ¬±√†¬Ω¬¥', '√†¬Ω¬¶', '√†¬ºƒ≠', '√†¬Ω¬ß', '√†¬ºƒ≠', '√†¬Ωƒß√†¬Ωƒ¶', '√†¬ºƒ≠', '√†¬Ω¬¢', '√†¬Ω¬≤', '√†¬Ωƒ¶', '√†¬ºƒ≠', '√†¬Ωƒ∂', '√†¬Ω¬º√†¬ºƒ≠', '√†¬Ω¬°', '√†¬Ω¬º', '√†¬Ωƒ≥', '√†¬ºƒ≠', '√†¬Ωƒ∂√†¬Ω≈Ç', '√†¬Ω¬≤√†¬ºƒ≠', '√†¬Ω¬¶', '√†¬æƒ≤', '√†¬Ωƒ≥', '√†¬ºƒ≠', '√†¬Ω¬°', '√†¬Ω¬≤', '√†¬Ωƒ§', '√†¬ºƒ≠', '√†¬Ωƒß', '√†¬Ω¬≤', '√†¬Ωƒ§', '√†¬ºƒ≠', '√†¬Ω¬¢', '√†¬Ω¬∫', '√†¬Ωƒ≥', '√†¬ºƒØ']\n",
      "Roundtrip successful: True\n",
      "\n",
      "‚ö†Ô∏è New tokenizer uses 20 more tokens.\n",
      "\n",
      "==== Test Example 3 ====\n",
      "\n",
      "\n",
      "--- Original Tokenizer Results ---\n",
      "Text: ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω¢‡æ£‡Ωò‡ºã‡Ωî‡ºã‡ΩÇ‡Ωâ‡Ω≤‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ωì‡ºã‡Ω°‡Ω∫‡ºã‡Ω§‡Ω∫‡Ω¶‡ºã‡Ω£‡æ∑‡ºã‡Ω£‡ºã‡Ω†‡ΩÇ‡æ±‡Ω¥‡Ω¢‡ºã‡Ω†‡ΩÇ‡æ≤‡Ωº‡ºã‡ΩÇ‡Ω≤‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ωò‡Ω∫‡Ωë‡ºã‡Ωì‡ºç ‡Ωë‡Ω∫‡ºã‡Ωì‡Ω¶‡ºã‡Ω†‡Ωë‡Ω≤‡ºã\n",
      "Token count: 69\n",
      "Tokens: ['‡Ωñ‡æ±‡ΩÑ', '‡ºã', '‡ΩÜ‡Ω¥‡Ωñ', '‡ºã', '‡ΩÄ‡æ±‡Ω≤', '‡ºã', '‡Ω¶‡Ω∫‡Ωò‡Ω¶', '‡ºã', '‡Ω¢‡æ£‡Ωò', '‡ºã', '‡Ωî', '‡ºã', '‡ΩÇ‡Ωâ‡Ω≤‡Ω¶', '‡ºã', '‡Ω°‡Ωº‡Ωë', '‡ºã', '‡Ω¢‡Ω∫‡Ωë', '‡ºç', 'ƒ†', '‡Ωñ‡æ±‡ΩÑ', '‡ºã', '‡ΩÜ‡Ω¥‡Ωñ', '‡ºã', '‡ΩÄ‡æ±‡Ω≤', '‡ºã', '‡Ω¶‡Ω∫‡Ωò‡Ω¶', '‡ºã', '‡Ω°‡Ωº‡Ωë', '‡ºã', '‡Ωì', '‡ºã', '‡Ω°‡Ω∫', '‡ºã', '‡Ω§‡Ω∫‡Ω¶', '‡ºã', '‡Ω£‡æ∑', '‡ºã', '‡Ω£', '‡ºã', '‡Ω†‡ΩÇ‡æ±‡Ω¥‡Ω¢', '‡ºã', '‡Ω†‡ΩÇ‡æ≤‡Ωº', '‡ºã', '‡ΩÇ‡Ω≤', '‡ºã', '‡Ω°‡Ωº‡Ωë', '‡ºã', '‡Ω¢‡Ω∫‡Ωë', '‡ºç', 'ƒ†', '‡Ωñ‡æ±‡ΩÑ', '‡ºã', '‡ΩÜ‡Ω¥‡Ωñ', '‡ºã', '‡ΩÄ‡æ±‡Ω≤', '‡ºã', '‡Ω¶‡Ω∫‡Ωò‡Ω¶', '‡ºã', '‡Ωò‡Ω∫‡Ωë', '‡ºã', '‡Ωì', '‡ºç', 'ƒ†', '‡Ωë‡Ω∫', '‡ºã', '‡Ωì‡Ω¶', '‡ºã', '‡Ω†‡Ωë‡Ω≤', '‡ºã']\n",
      "Roundtrip successful: True\n",
      "\n",
      "--- Trained Tokenizer Results ---\n",
      "Text: ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω¢‡æ£‡Ωò‡ºã‡Ωî‡ºã‡ΩÇ‡Ωâ‡Ω≤‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ωì‡ºã‡Ω°‡Ω∫‡ºã‡Ω§‡Ω∫‡Ω¶‡ºã‡Ω£‡æ∑‡ºã‡Ω£‡ºã‡Ω†‡ΩÇ‡æ±‡Ω¥‡Ω¢‡ºã‡Ω†‡ΩÇ‡æ≤‡Ωº‡ºã‡ΩÇ‡Ω≤‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ωò‡Ω∫‡Ωë‡ºã‡Ωì‡ºç ‡Ωë‡Ω∫‡ºã‡Ωì‡Ω¶‡ºã‡Ω†‡Ωë‡Ω≤‡ºã\n",
      "Token count: 104\n",
      "Tokens: ['√†¬Ωƒ∏', '√†¬æ¬±', '√†¬Ωƒ¶', '√†¬ºƒ≠', '√†¬Ωƒ®', '√†¬Ω¬¥', '√†¬Ωƒ∏', '√†¬ºƒ≠', '√†¬Ωƒ¢', '√†¬æ¬±√†¬Ω¬≤√†¬ºƒ≠', '√†¬Ω¬¶', '√†¬Ω¬∫', '√†¬Ωƒ∫√†¬Ω¬¶', '√†¬ºƒ≠', '√†¬Ω¬¢', '√†¬æ¬£', '√†¬Ωƒ∫', '√†¬ºƒ≠', '√†¬Ωƒ∂', '√†¬ºƒ≠', '√†¬Ωƒ§√†¬Ωƒ´', '√†¬Ω¬≤', '√†¬Ω¬¶', '√†¬ºƒ≠', '√†¬Ω¬°', '√†¬Ω¬º', '√†¬Ωƒ≥', '√†¬ºƒ≠', '√†¬Ω¬¢', '√†¬Ω¬∫', '√†¬Ωƒ≥', '√†¬ºƒØ', 'ƒ†√†¬Ωƒ∏', '√†¬æ¬±', '√†¬Ωƒ¶', '√†¬ºƒ≠', '√†¬Ωƒ®', '√†¬Ω¬¥', '√†¬Ωƒ∏', '√†¬ºƒ≠', '√†¬Ωƒ¢', '√†¬æ¬±√†¬Ω¬≤√†¬ºƒ≠', '√†¬Ω¬¶', '√†¬Ω¬∫', '√†¬Ωƒ∫√†¬Ω¬¶', '√†¬ºƒ≠', '√†¬Ω¬°', '√†¬Ω¬º', '√†¬Ωƒ≥', '√†¬ºƒ≠', '√†¬Ωƒµ', '√†¬ºƒ≠', '√†¬Ω¬°', '√†¬Ω¬∫√†¬ºƒ≠', '√†¬Ω¬§', '√†¬Ω¬∫', '√†¬Ω¬¶', '√†¬ºƒ≠', '√†¬Ω¬£', '√†¬æ¬∑√†¬ºƒ≠', '√†¬Ω¬£', '√†¬ºƒ≠', '√†¬Ω≈Ç√†¬Ωƒ§', '√†¬æ¬±√†¬Ω¬¥', '√†¬Ω¬¢', '√†¬ºƒ≠', '√†¬Ω≈Ç√†¬Ωƒ§', '√†¬æ¬≤√†¬Ω¬º√†¬ºƒ≠', '√†¬Ωƒ§', '√†¬Ω¬≤√†¬ºƒ≠', '√†¬Ω¬°', '√†¬Ω¬º', '√†¬Ωƒ≥', '√†¬ºƒ≠', '√†¬Ω¬¢', '√†¬Ω¬∫', '√†¬Ωƒ≥', '√†¬ºƒØ', 'ƒ†√†¬Ωƒ∏', '√†¬æ¬±', '√†¬Ωƒ¶', '√†¬ºƒ≠', '√†¬Ωƒ®', '√†¬Ω¬¥', '√†¬Ωƒ∏', '√†¬ºƒ≠', '√†¬Ωƒ¢', '√†¬æ¬±√†¬Ω¬≤√†¬ºƒ≠', '√†¬Ω¬¶', '√†¬Ω¬∫', '√†¬Ωƒ∫√†¬Ω¬¶', '√†¬ºƒ≠', '√†¬Ωƒ∫', '√†¬Ω¬∫', '√†¬Ωƒ≥', '√†¬ºƒ≠', '√†¬Ωƒµ', '√†¬ºƒØ', 'ƒ†√†¬Ωƒ≥', '√†¬Ω¬∫√†¬ºƒ≠', '√†¬Ωƒµ√†¬Ω¬¶', '√†¬ºƒ≠', '√†¬Ω≈Ç√†¬Ωƒ≥', '√†¬Ω¬≤√†¬ºƒ≠']\n",
      "Roundtrip successful: True\n",
      "\n",
      "‚ö†Ô∏è New tokenizer uses 35 more tokens.\n"
     ]
    }
   ],
   "source": [
    "# Compare tokenization between original and new tokenizer\n",
    "for i, example in enumerate(test_examples):\n",
    "    print(f\"\\n==== Test Example {i+1} ====\\n\")\n",
    "    \n",
    "    # Test original tokenizer\n",
    "    orig_count = analyze_tokenization(tokenizer, example, \"Original Tokenizer\")\n",
    "    \n",
    "    # Test new tokenizer\n",
    "    new_count = analyze_tokenization(new_tokenizer, example, \"Trained Tokenizer\")\n",
    "    \n",
    "    # Compare\n",
    "    if new_count < orig_count:\n",
    "        print(f\"\\n‚ú® Improvement: {orig_count - new_count} fewer tokens used!\")\n",
    "    elif new_count == orig_count:\n",
    "        print(f\"\\nüîÑ Same number of tokens used.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è New tokenizer uses {new_count - orig_count} more tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze the New Vocabulary\n",
    "\n",
    "Let's analyze what new tokens were added and check for Tibetan-specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2695 new tokens to the vocabulary\n",
      "Added 0 new Tibetan tokens\n"
     ]
    }
   ],
   "source": [
    "# Get vocabularies\n",
    "old_vocab = tokenizer.get_vocab()\n",
    "new_vocab = new_tokenizer.get_vocab()\n",
    "\n",
    "# Find new tokens\n",
    "new_tokens = [token for token in new_vocab.keys() if token not in old_vocab]\n",
    "print(f\"Added {len(new_tokens)} new tokens to the vocabulary\")\n",
    "\n",
    "# Find Tibetan tokens\n",
    "tibetan_range = (0x0F00, 0x0FFF)  # Unicode range for Tibetan\n",
    "new_tibetan_tokens = [token for token in new_tokens \n",
    "                      if any(ord(c) >= tibetan_range[0] and ord(c) <= tibetan_range[1] for c in token)]\n",
    "\n",
    "print(f\"Added {len(new_tibetan_tokens)} new Tibetan tokens\")\n",
    "\n",
    "# Show some examples\n",
    "if new_tibetan_tokens:\n",
    "    print(\"\\nSample new Tibetan tokens:\")\n",
    "    for token in new_tibetan_tokens[:20]:  # Show up to 20\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Trained Tokenizer\n",
    "\n",
    "Save the new tokenizer for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Trained tokenizer saved to: data/whisper_tibetan_tokenizer_retrained\n",
      "‚úÖ Successfully loaded tokenizer with 3059 tokens\n"
     ]
    }
   ],
   "source": [
    "# Define output directory\n",
    "OUTPUT_DIR = \"data/whisper_tibetan_tokenizer_retrained\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the tokenizer\n",
    "new_tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"üíæ Trained tokenizer saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Test loading it back\n",
    "loaded_tokenizer = PreTrainedTokenizerFast.from_pretrained(OUTPUT_DIR)\n",
    "print(f\"‚úÖ Successfully loaded tokenizer with {len(loaded_tokenizer)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "Now that you've trained a tokenizer for Tibetan, here's how you can use it with a Whisper model:\n",
    "\n",
    "```python\n",
    "from transformers import WhisperForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# Load your trained tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"data/whisper_tibetan_tokenizer\")\n",
    "\n",
    "# Load a Whisper model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# Resize the token embeddings to match your tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Now you can fine-tune or use the model with your tokenizer\n",
    "```\n",
    "\n",
    "Remember to always resize the token embeddings of the model after loading your custom tokenizer!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
