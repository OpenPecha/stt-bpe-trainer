{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Fast Whisper Tokenizer for Tibetan\n",
    "\n",
    "This notebook demonstrates how to train a Whisper tokenizer for Tibetan text using the `PreTrainedTokenizerFast` class, which correctly supports the `train_new_from_iterator` method.\n",
    "\n",
    "We'll follow these steps:\n",
    "1. Load a fast tokenizer from the Whisper model\n",
    "2. Prepare a Tibetan text corpus\n",
    "3. Train the tokenizer on the Tibetan corpus\n",
    "4. Test and evaluate the tokenizer\n",
    "5. Save the trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/gangagyatso/.local/lib/python3.10/site-packages (4.56.1)\n",
      "Requirement already satisfied: tokenizers in /home/gangagyatso/.local/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: tqdm in /home/gangagyatso/.local/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/gangagyatso/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gangagyatso/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/gangagyatso/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/gangagyatso/.local/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gangagyatso/.local/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gangagyatso/.local/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gangagyatso/.local/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install transformers tokenizers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from transformers import PreTrainedTokenizerFast, WhisperProcessor\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Base Fast Tokenizer\n",
    "\n",
    "We'll load the tokenizer from the Whisper model using `PreTrainedTokenizerFast` which supports the `train_new_from_iterator` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Loading fast tokenizer from /home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe/bpe_tokenizer.json...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Calling PreTrainedTokenizerFast.from_pretrained() with the path to a single file or url is not supported for this tokenizer. Use a model identifier or the path to a directory instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the fast tokenizer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš€ Loading fast tokenizer from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_MODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPreTrainedTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Fast tokenizer loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Check vocabulary size\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1934\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(pretrained_model_name_or_path) \u001b[38;5;129;01mor\u001b[39;00m is_remote_url(pretrained_model_name_or_path):\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_files_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 1934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1935\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_pretrained() with the path to a single file or url is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1936\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported for this tokenizer. Use a model identifier or the path to a directory instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1937\u001b[0m         )\n\u001b[1;32m   1938\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_pretrained() with the path to a single file or url is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1940\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be possible anymore in v5. Use a model identifier or the path to a directory instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1941\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1942\u001b[0m     )\n\u001b[1;32m   1943\u001b[0m     file_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_files_names\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Calling PreTrainedTokenizerFast.from_pretrained() with the path to a single file or url is not supported for this tokenizer. Use a model identifier or the path to a directory instead."
     ]
    }
   ],
   "source": [
    "# Define the base model\n",
    "BASE_MODEL = \"/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe/bpe_tokenizer.json\"\n",
    "\n",
    "# Load the fast tokenizer\n",
    "print(f\"ğŸš€ Loading fast tokenizer from {BASE_MODEL}...\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(BASE_MODEL)\n",
    "print(\"âœ… Fast tokenizer loaded successfully\")\n",
    "\n",
    "# Check vocabulary size\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output directory\n",
    "OUTPUT_DIR = \"data/whisper_tokenizer\"\n",
    "\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Trained tokenizer saved to: data/whisper_tokenizer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"ğŸ’¾ Trained tokenizer saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully loaded tokenizer with 51865 tokens\n"
     ]
    }
   ],
   "source": [
    "# Test loading it back\n",
    "loaded_tokenizer = PreTrainedTokenizerFast.from_pretrained(OUTPUT_DIR)\n",
    "print(f\"âœ… Successfully loaded tokenizer with {len(loaded_tokenizer)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the Tibetan Corpus\n",
    "\n",
    "Create an iterator for the Tibetan corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found corpus file: data/corpus/mergedcorpus.txt\n",
      "\n",
      "Sample of corpus:\n",
      "Line 1: à½¨à½ºà¼ à½¦à½´à½ à½²à¼‹à½¦à¾£à½„à¼‹à½–à½¢à¼‹à½‘à½ºà¼‹à½¡à½¼à½‘à¼‹à½“à¼‹\n",
      "Line 2: à½¦à½ºà½˜à½¦à¼‹à½…à½“à¼‹à½à½˜à½¦à¼‹à½…à½‘à¼‹à½€à¾±à½²à¼‹à½šà½–à¼‹à½£à¼‹à½‚à½à½“à¼‹à½–à½¦à½˜à¼‹à½šà½¢à¼‹à½‘à½´à½¦à¼‹à½–à½‘à½‚à¼‹à½ à½›à½²à½“à¼‹à½¡à½¼à½‘à¼‹à½˜à¼‹à½¢à½ºà½‘à¼‹\n",
      "Line 3: à½¢à½„à¼‹à½à½¼à½‚à¼‹à½£à¼‹à½‘à½„à½¼à½¦à¼‹à½¦à½´à¼‹à½¦à¾™à½²à½„à¼‹à½¢à¾—à½ºà¼‹à½¡à½²à½¦à¼‹à½ à½–à¾²à½ºà½£à¼‹à½–à¼‹à½¡à½¼à½„à¼‹à½‚à½²à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼‹\n"
     ]
    }
   ],
   "source": [
    "# Path to your Tibetan corpus file\n",
    "CORPUS_FILE = \"data/corpus/mergedcorpus.txt\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(CORPUS_FILE):\n",
    "    print(f\"âŒ Corpus file not found: {CORPUS_FILE}\")\n",
    "else:\n",
    "    print(f\"âœ… Found corpus file: {CORPUS_FILE}\")\n",
    "    \n",
    "    # Look at a sample of the corpus\n",
    "    with open(CORPUS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        sample_lines = [next(f).strip() for _ in range(3) if f]\n",
    "    \n",
    "    print(\"\\nSample of corpus:\")\n",
    "    for i, line in enumerate(sample_lines):\n",
    "        print(f\"Line {i+1}: {line[:100]}...\" if len(line) > 100 else f\"Line {i+1}: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define corpus iterator function\n",
    "def corpus_iterator():\n",
    "    with open(CORPUS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                yield line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Initial Tokenizer on Tibetan\n",
    "\n",
    "Let's first see how the original tokenizer handles Tibetan text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mà½–à½‘à½ºà¼‹à½£à½ºà½‚à½¦à¼‹\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(sentence)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mids)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tokens'"
     ]
    }
   ],
   "source": [
    "# Test encoding a sentence\n",
    "sentence = \"à½–à½‘à½ºà¼‹à½£à½ºà½‚à½¦à¼‹\"\n",
    "output = tokenizer.encode(sentence)\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"IDs:\", output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: [50258, 50363, 156, 121, 244, 156, 121, 239, 156, 121, 118, 156, 120, 233, 156, 121, 96, 156, 121, 118, 156, 121, 224, 156, 121, 99, 156, 120, 233, 50257]\n",
      "Decoded Text: <|startoftranscript|><|notimestamps|>à½–à½‘à½ºà¼‹à½£à½ºà½‚à½¦à¼‹<|endoftext|>\n",
      "token len:  30\n"
     ]
    }
   ],
   "source": [
    "# The list of token IDs you want to decode\n",
    "ids_to_decode = [50258, 50363, 156, 121, 244, 156, 121, 239, 156, 121, 118, 156, 120, 233, 156, 121, 96, 156, 121, 118, 156, 121, 224, 156, 121, 99, 156, 120, 233, 50257]\n",
    "\n",
    "# Use the decode method to convert IDs back to text\n",
    "decoded_text = loaded_tokenizer.decode(ids_to_decode)\n",
    "\n",
    "print(f\"IDs: {ids_to_decode}\")\n",
    "print(f\"Decoded Text: {decoded_text}\")\n",
    "print(\"token len: \", len(ids_to_decode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokenizer results:\n",
      "Text: à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½¢à¾£à½˜à¼‹à½”à¼‹à½‚à½‰à½²à½¦à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼\n",
      "Token count: 108\n",
      "Tokens: ['Ã ', 'Â½', 'Ä¸', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Ä¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¨', 'Ã ', 'Â½', 'Â´', 'Ã ', 'Â½', 'Ä¸', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¢', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â½', 'Äº', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¢', 'Ã ', 'Â¾', 'Â£', 'Ã ', 'Â½', 'Äº', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¶', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¤', 'Ã ', 'Â½', 'Ä«', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¡', 'Ã ', 'Â½', 'Â¼', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¢', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä¯']\n",
      "Token IDs: [50258, 50363, 156, 121, 244, 156, 122, 109, 156, 121, 226, 156, 120, 233, 156, 121, 228, 156, 121, 112, 156, 121, 244, 156, 120, 233, 156, 121, 222, 156, 122, 109, 156, 121, 110, 156, 120, 233, 156, 121, 99, 156, 121, 118, 156, 121, 246, 156, 121, 99, 156, 120, 233, 156, 121, 95, 156, 122, 96, 156, 121, 246, 156, 120, 233, 156, 121, 242, 156, 120, 233, 156, 121, 224, 156, 121, 231, 156, 121, 110, 156, 121, 99, 156, 120, 233, 156, 121, 94, 156, 121, 120, 156, 121, 239, 156, 120, 233, 156, 121, 95, 156, 121, 118, 156, 121, 239, 156, 120, 235, 50257]\n",
      "\n",
      "Roundtrip: True\n",
      "Decoded text: <|startoftranscript|><|notimestamps|>à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½¢à¾£à½˜à¼‹à½”à¼‹à½‚à½‰à½²à½¦à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Example Tibetan text\n",
    "tibetan_example = \"à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½¢à¾£à½˜à¼‹à½”à¼‹à½‚à½‰à½²à½¦à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼\"\n",
    "\n",
    "# Tokenize with the original tokenizer\n",
    "tokens = tokenizer.tokenize(tibetan_example)\n",
    "token_ids = tokenizer.encode(tibetan_example)\n",
    "\n",
    "print(\"Original tokenizer results:\")\n",
    "print(f\"Text: {tibetan_example}\")\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Check roundtrip\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"\\nRoundtrip: {tibetan_example in decoded}\")\n",
    "print(f\"Decoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Tokenizer on Tibetan Corpus\n",
    "\n",
    "Now we'll train the tokenizer on our Tibetan corpus to improve its handling of Tibetan text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Original vocabulary size: 51865\n",
      "ğŸ¯ Target vocabulary size: 61865\n",
      "â³ Starting tokenizer training... This may take a few minutes.\n",
      "\n",
      "\n",
      "\n",
      "âœ… Training complete!\n",
      "ğŸ“ˆ New vocabulary size: 3059\n"
     ]
    }
   ],
   "source": [
    "# Set target vocabulary size\n",
    "TARGET_VOCAB_SIZE = len(tokenizer) + 10000\n",
    "\n",
    "print(f\"ğŸ§  Original vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"ğŸ¯ Target vocabulary size: {TARGET_VOCAB_SIZE}\")\n",
    "\n",
    "print(\"â³ Starting tokenizer training... This may take a few minutes.\")\n",
    "\n",
    "# Train the tokenizer\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(corpus_iterator(), vocab_size=TARGET_VOCAB_SIZE, )\n",
    "\n",
    "print(\"âœ… Training complete!\")\n",
    "print(f\"ğŸ“ˆ New vocabulary size: {len(new_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Trained Tokenizer\n",
    "\n",
    "Let's check how the newly trained tokenizer handles Tibetan text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test examples\n",
    "test_examples = [\n",
    "    \"à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½¢à¾£à½˜à¼‹à½”à¼‹à½‚à½‰à½²à½¦à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼\",  # Simple sentence\n",
    "    \"à½–à½¼à½‘à¼‹à½€à¾±à½²à¼‹à½¦à¾à½‘à¼‹à½¡à½²à½‚à¼‹à½“à½²à¼‹à½£à½¼à¼‹à½¢à¾’à¾±à½´à½¦à¼‹à½§à¼‹à½…à½„à¼‹à½¢à½²à½„à¼‹à½”à½¼à¼‹à½¡à½¼à½‘à¼‹à½”à½ à½²à¼‹à½¦à¾à½‘à¼‹à½¡à½²à½‚à¼‹à½…à½²à½‚à¼‹à½¢à½ºà½‘à¼\",  # Another example\n",
    "    \"à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½¢à¾£à½˜à¼‹à½”à¼‹à½‚à½‰à½²à½¦à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼ à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½¡à½¼à½‘à¼‹à½“à¼‹à½¡à½ºà¼‹à½¤à½ºà½¦à¼‹à½£à¾·à¼‹à½£à¼‹à½ à½‚à¾±à½´à½¢à¼‹à½ à½‚à¾²à½¼à¼‹à½‚à½²à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼ à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½˜à½ºà½‘à¼‹à½“à¼ à½‘à½ºà¼‹à½“à½¦à¼‹à½ à½‘à½²à¼‹\"\n",
    "]\n",
    "\n",
    "# Function to analyze tokenization\n",
    "def analyze_tokenization(tokenizer, text, name=\"Tokenizer\"):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    decoded = tokenizer.decode(token_ids)\n",
    "    \n",
    "    print(f\"\\n--- {name} Results ---\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Token count: {len(tokens)}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Roundtrip successful: {text in decoded}\")\n",
    "    \n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Test Example 1 ====\n",
      "\n",
      "\n",
      "--- Original Tokenizer Results ---\n",
      "Text: à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½¢à¾£à½˜à¼‹à½”à¼‹à½‚à½‰à½²à½¦à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼\n",
      "Token count: 108\n",
      "Tokens: ['Ã ', 'Â½', 'Ä¸', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Ä¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¨', 'Ã ', 'Â½', 'Â´', 'Ã ', 'Â½', 'Ä¸', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¢', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â½', 'Äº', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¢', 'Ã ', 'Â¾', 'Â£', 'Ã ', 'Â½', 'Äº', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¶', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¤', 'Ã ', 'Â½', 'Ä«', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¡', 'Ã ', 'Â½', 'Â¼', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¢', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä¯']\n",
      "Roundtrip successful: True\n",
      "\n",
      "--- Trained Tokenizer Results ---\n",
      "Text: à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½¢à¾£à½˜à¼‹à½”à¼‹à½‚à½‰à½²à½¦à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼\n",
      "Token count: 32\n",
      "Tokens: ['Ã Â½Ä¸', 'Ã Â¾Â±', 'Ã Â½Ä¦', 'Ã Â¼Ä­', 'Ã Â½Ä¨', 'Ã Â½Â´', 'Ã Â½Ä¸', 'Ã Â¼Ä­', 'Ã Â½Ä¢', 'Ã Â¾Â±Ã Â½Â²Ã Â¼Ä­', 'Ã Â½Â¦', 'Ã Â½Âº', 'Ã Â½ÄºÃ Â½Â¦', 'Ã Â¼Ä­', 'Ã Â½Â¢', 'Ã Â¾Â£', 'Ã Â½Äº', 'Ã Â¼Ä­', 'Ã Â½Ä¶', 'Ã Â¼Ä­', 'Ã Â½Ä¤Ã Â½Ä«', 'Ã Â½Â²', 'Ã Â½Â¦', 'Ã Â¼Ä­', 'Ã Â½Â¡', 'Ã Â½Â¼', 'Ã Â½Ä³', 'Ã Â¼Ä­', 'Ã Â½Â¢', 'Ã Â½Âº', 'Ã Â½Ä³', 'Ã Â¼Ä¯']\n",
      "Roundtrip successful: True\n",
      "\n",
      "âœ¨ Improvement: 76 fewer tokens used!\n",
      "\n",
      "==== Test Example 2 ====\n",
      "\n",
      "\n",
      "--- Original Tokenizer Results ---\n",
      "Text: à½–à½¼à½‘à¼‹à½€à¾±à½²à¼‹à½¦à¾à½‘à¼‹à½¡à½²à½‚à¼‹à½“à½²à¼‹à½£à½¼à¼‹à½¢à¾’à¾±à½´à½¦à¼‹à½§à¼‹à½…à½„à¼‹à½¢à½²à½„à¼‹à½”à½¼à¼‹à½¡à½¼à½‘à¼‹à½”à½ à½²à¼‹à½¦à¾à½‘à¼‹à½¡à½²à½‚à¼‹à½…à½²à½‚à¼‹à½¢à½ºà½‘à¼\n",
      "Token count: 192\n",
      "Tokens: ['Ã ', 'Â½', 'Ä¸', 'Ã ', 'Â½', 'Â¼', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¢', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¾', 'Ä²', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¡', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â½', 'Ä¤', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Äµ', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â£', 'Ã ', 'Â½', 'Â¼', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¢', 'Ã ', 'Â¾', 'Ä´', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Â´', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â§', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä§', 'Ã ', 'Â½', 'Ä¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¢', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â½', 'Ä¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¶', 'Ã ', 'Â½', 'Â¼', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¡', 'Ã ', 'Â½', 'Â¼', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¶', 'Ã ', 'Â½', 'Å‚', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¾', 'Ä²', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¡', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â½', 'Ä¤', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä§', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â½', 'Ä¤', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¢', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä¯']\n",
      "Roundtrip successful: True\n",
      "\n",
      "--- Trained Tokenizer Results ---\n",
      "Text: à½–à½¼à½‘à¼‹à½€à¾±à½²à¼‹à½¦à¾à½‘à¼‹à½¡à½²à½‚à¼‹à½“à½²à¼‹à½£à½¼à¼‹à½¢à¾’à¾±à½´à½¦à¼‹à½§à¼‹à½…à½„à¼‹à½¢à½²à½„à¼‹à½”à½¼à¼‹à½¡à½¼à½‘à¼‹à½”à½ à½²à¼‹à½¦à¾à½‘à¼‹à½¡à½²à½‚à¼‹à½…à½²à½‚à¼‹à½¢à½ºà½‘à¼\n",
      "Token count: 54\n",
      "Tokens: ['Ã Â½Ä¸', 'Ã Â½Â¼', 'Ã Â½Ä³', 'Ã Â¼Ä­', 'Ã Â½Ä¢', 'Ã Â¾Â±Ã Â½Â²Ã Â¼Ä­', 'Ã Â½Â¦', 'Ã Â¾Ä²', 'Ã Â½Ä³', 'Ã Â¼Ä­', 'Ã Â½Â¡', 'Ã Â½Â²', 'Ã Â½Ä¤', 'Ã Â¼Ä­', 'Ã Â½Äµ', 'Ã Â½Â²Ã Â¼Ä­', 'Ã Â½Â£', 'Ã Â½Â¼Ã Â¼Ä­', 'Ã Â½Â¢', 'Ã Â¾Ä´Ã Â¾Â±Ã Â½Â´', 'Ã Â½Â¦', 'Ã Â¼Ä­', 'Ã Â½Â§', 'Ã Â¼Ä­', 'Ã Â½Ä§Ã Â½Ä¦', 'Ã Â¼Ä­', 'Ã Â½Â¢', 'Ã Â½Â²', 'Ã Â½Ä¦', 'Ã Â¼Ä­', 'Ã Â½Ä¶', 'Ã Â½Â¼Ã Â¼Ä­', 'Ã Â½Â¡', 'Ã Â½Â¼', 'Ã Â½Ä³', 'Ã Â¼Ä­', 'Ã Â½Ä¶Ã Â½Å‚', 'Ã Â½Â²Ã Â¼Ä­', 'Ã Â½Â¦', 'Ã Â¾Ä²', 'Ã Â½Ä³', 'Ã Â¼Ä­', 'Ã Â½Â¡', 'Ã Â½Â²', 'Ã Â½Ä¤', 'Ã Â¼Ä­', 'Ã Â½Ä§', 'Ã Â½Â²', 'Ã Â½Ä¤', 'Ã Â¼Ä­', 'Ã Â½Â¢', 'Ã Â½Âº', 'Ã Â½Ä³', 'Ã Â¼Ä¯']\n",
      "Roundtrip successful: True\n",
      "\n",
      "âœ¨ Improvement: 138 fewer tokens used!\n",
      "\n",
      "==== Test Example 3 ====\n",
      "\n",
      "\n",
      "--- Original Tokenizer Results ---\n",
      "Text: à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½¢à¾£à½˜à¼‹à½”à¼‹à½‚à½‰à½²à½¦à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼ à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½¡à½¼à½‘à¼‹à½“à¼‹à½¡à½ºà¼‹à½¤à½ºà½¦à¼‹à½£à¾·à¼‹à½£à¼‹à½ à½‚à¾±à½´à½¢à¼‹à½ à½‚à¾²à½¼à¼‹à½‚à½²à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼ à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½˜à½ºà½‘à¼‹à½“à¼ à½‘à½ºà¼‹à½“à½¦à¼‹à½ à½‘à½²à¼‹\n",
      "Token count: 381\n",
      "Tokens: ['Ã ', 'Â½', 'Ä¸', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Ä¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¨', 'Ã ', 'Â½', 'Â´', 'Ã ', 'Â½', 'Ä¸', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¢', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â½', 'Äº', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¢', 'Ã ', 'Â¾', 'Â£', 'Ã ', 'Â½', 'Äº', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¶', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¤', 'Ã ', 'Â½', 'Ä«', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¡', 'Ã ', 'Â½', 'Â¼', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¢', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä¯', 'Ä ', 'Ã ', 'Â½', 'Ä¸', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Ä¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¨', 'Ã ', 'Â½', 'Â´', 'Ã ', 'Â½', 'Ä¸', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¢', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â½', 'Äº', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¡', 'Ã ', 'Â½', 'Â¼', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Äµ', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¡', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¤', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â£', 'Ã ', 'Â¾', 'Â·', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â£', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Å‚', 'Ã ', 'Â½', 'Ä¤', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Â´', 'Ã ', 'Â½', 'Â¢', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Å‚', 'Ã ', 'Â½', 'Ä¤', 'Ã ', 'Â¾', 'Â²', 'Ã ', 'Â½', 'Â¼', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¤', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¡', 'Ã ', 'Â½', 'Â¼', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¢', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä¯', 'Ä ', 'Ã ', 'Â½', 'Ä¸', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Ä¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¨', 'Ã ', 'Â½', 'Â´', 'Ã ', 'Â½', 'Ä¸', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Ä¢', 'Ã ', 'Â¾', 'Â±', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â½', 'Äº', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Äº', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Äµ', 'Ã ', 'Â¼', 'Ä¯', 'Ä ', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â½', 'Âº', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Äµ', 'Ã ', 'Â½', 'Â¦', 'Ã ', 'Â¼', 'Ä­', 'Ã ', 'Â½', 'Å‚', 'Ã ', 'Â½', 'Ä³', 'Ã ', 'Â½', 'Â²', 'Ã ', 'Â¼', 'Ä­']\n",
      "Roundtrip successful: True\n",
      "\n",
      "--- Trained Tokenizer Results ---\n",
      "Text: à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½¢à¾£à½˜à¼‹à½”à¼‹à½‚à½‰à½²à½¦à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼ à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½¡à½¼à½‘à¼‹à½“à¼‹à½¡à½ºà¼‹à½¤à½ºà½¦à¼‹à½£à¾·à¼‹à½£à¼‹à½ à½‚à¾±à½´à½¢à¼‹à½ à½‚à¾²à½¼à¼‹à½‚à½²à¼‹à½¡à½¼à½‘à¼‹à½¢à½ºà½‘à¼ à½–à¾±à½„à¼‹à½†à½´à½–à¼‹à½€à¾±à½²à¼‹à½¦à½ºà½˜à½¦à¼‹à½˜à½ºà½‘à¼‹à½“à¼ à½‘à½ºà¼‹à½“à½¦à¼‹à½ à½‘à½²à¼‹\n",
      "Token count: 104\n",
      "Tokens: ['Ã Â½Ä¸', 'Ã Â¾Â±', 'Ã Â½Ä¦', 'Ã Â¼Ä­', 'Ã Â½Ä¨', 'Ã Â½Â´', 'Ã Â½Ä¸', 'Ã Â¼Ä­', 'Ã Â½Ä¢', 'Ã Â¾Â±Ã Â½Â²Ã Â¼Ä­', 'Ã Â½Â¦', 'Ã Â½Âº', 'Ã Â½ÄºÃ Â½Â¦', 'Ã Â¼Ä­', 'Ã Â½Â¢', 'Ã Â¾Â£', 'Ã Â½Äº', 'Ã Â¼Ä­', 'Ã Â½Ä¶', 'Ã Â¼Ä­', 'Ã Â½Ä¤Ã Â½Ä«', 'Ã Â½Â²', 'Ã Â½Â¦', 'Ã Â¼Ä­', 'Ã Â½Â¡', 'Ã Â½Â¼', 'Ã Â½Ä³', 'Ã Â¼Ä­', 'Ã Â½Â¢', 'Ã Â½Âº', 'Ã Â½Ä³', 'Ã Â¼Ä¯', 'Ä Ã Â½Ä¸', 'Ã Â¾Â±', 'Ã Â½Ä¦', 'Ã Â¼Ä­', 'Ã Â½Ä¨', 'Ã Â½Â´', 'Ã Â½Ä¸', 'Ã Â¼Ä­', 'Ã Â½Ä¢', 'Ã Â¾Â±Ã Â½Â²Ã Â¼Ä­', 'Ã Â½Â¦', 'Ã Â½Âº', 'Ã Â½ÄºÃ Â½Â¦', 'Ã Â¼Ä­', 'Ã Â½Â¡', 'Ã Â½Â¼', 'Ã Â½Ä³', 'Ã Â¼Ä­', 'Ã Â½Äµ', 'Ã Â¼Ä­', 'Ã Â½Â¡', 'Ã Â½ÂºÃ Â¼Ä­', 'Ã Â½Â¤', 'Ã Â½Âº', 'Ã Â½Â¦', 'Ã Â¼Ä­', 'Ã Â½Â£', 'Ã Â¾Â·Ã Â¼Ä­', 'Ã Â½Â£', 'Ã Â¼Ä­', 'Ã Â½Å‚Ã Â½Ä¤', 'Ã Â¾Â±Ã Â½Â´', 'Ã Â½Â¢', 'Ã Â¼Ä­', 'Ã Â½Å‚Ã Â½Ä¤', 'Ã Â¾Â²Ã Â½Â¼Ã Â¼Ä­', 'Ã Â½Ä¤', 'Ã Â½Â²Ã Â¼Ä­', 'Ã Â½Â¡', 'Ã Â½Â¼', 'Ã Â½Ä³', 'Ã Â¼Ä­', 'Ã Â½Â¢', 'Ã Â½Âº', 'Ã Â½Ä³', 'Ã Â¼Ä¯', 'Ä Ã Â½Ä¸', 'Ã Â¾Â±', 'Ã Â½Ä¦', 'Ã Â¼Ä­', 'Ã Â½Ä¨', 'Ã Â½Â´', 'Ã Â½Ä¸', 'Ã Â¼Ä­', 'Ã Â½Ä¢', 'Ã Â¾Â±Ã Â½Â²Ã Â¼Ä­', 'Ã Â½Â¦', 'Ã Â½Âº', 'Ã Â½ÄºÃ Â½Â¦', 'Ã Â¼Ä­', 'Ã Â½Äº', 'Ã Â½Âº', 'Ã Â½Ä³', 'Ã Â¼Ä­', 'Ã Â½Äµ', 'Ã Â¼Ä¯', 'Ä Ã Â½Ä³', 'Ã Â½ÂºÃ Â¼Ä­', 'Ã Â½ÄµÃ Â½Â¦', 'Ã Â¼Ä­', 'Ã Â½Å‚Ã Â½Ä³', 'Ã Â½Â²Ã Â¼Ä­']\n",
      "Roundtrip successful: True\n",
      "\n",
      "âœ¨ Improvement: 277 fewer tokens used!\n"
     ]
    }
   ],
   "source": [
    "# Compare tokenization between original and new tokenizer\n",
    "for i, example in enumerate(test_examples):\n",
    "    print(f\"\\n==== Test Example {i+1} ====\\n\")\n",
    "    \n",
    "    # Test original tokenizer\n",
    "    orig_count = analyze_tokenization(tokenizer, example, \"Original Tokenizer\")\n",
    "    \n",
    "    # Test new tokenizer\n",
    "    new_count = analyze_tokenization(new_tokenizer, example, \"Trained Tokenizer\")\n",
    "    \n",
    "    # Compare\n",
    "    if new_count < orig_count:\n",
    "        print(f\"\\nâœ¨ Improvement: {orig_count - new_count} fewer tokens used!\")\n",
    "    elif new_count == orig_count:\n",
    "        print(f\"\\nğŸ”„ Same number of tokens used.\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ New tokenizer uses {new_count - orig_count} more tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze the New Vocabulary\n",
    "\n",
    "Let's analyze what new tokens were added and check for Tibetan-specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2695 new tokens to the vocabulary\n",
      "Added 0 new Tibetan tokens\n"
     ]
    }
   ],
   "source": [
    "# Get vocabularies\n",
    "old_vocab = tokenizer.get_vocab()\n",
    "new_vocab = new_tokenizer.get_vocab()\n",
    "\n",
    "# Find new tokens\n",
    "new_tokens = [token for token in new_vocab.keys() if token not in old_vocab]\n",
    "print(f\"Added {len(new_tokens)} new tokens to the vocabulary\")\n",
    "\n",
    "# Find Tibetan tokens\n",
    "tibetan_range = (0x0F00, 0x0FFF)  # Unicode range for Tibetan\n",
    "new_tibetan_tokens = [token for token in new_tokens \n",
    "                      if any(ord(c) >= tibetan_range[0] and ord(c) <= tibetan_range[1] for c in token)]\n",
    "\n",
    "print(f\"Added {len(new_tibetan_tokens)} new Tibetan tokens\")\n",
    "\n",
    "# Show some examples\n",
    "if new_tibetan_tokens:\n",
    "    print(\"\\nSample new Tibetan tokens:\")\n",
    "    for token in new_tibetan_tokens[:20]:  # Show up to 20\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Trained Tokenizer\n",
    "\n",
    "Save the new tokenizer for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Trained tokenizer saved to: data/whisper_latin_tibetan_tokenizer\n",
      "âœ… Successfully loaded tokenizer with 3059 tokens\n"
     ]
    }
   ],
   "source": [
    "# Define output directory\n",
    "OUTPUT_DIR = \"data/whisper_latin_tibetan_tokenizer\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the tokenizer\n",
    "new_tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"ğŸ’¾ Trained tokenizer saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Test loading it back\n",
    "loaded_tokenizer = PreTrainedTokenizerFast.from_pretrained(OUTPUT_DIR)\n",
    "print(f\"âœ… Successfully loaded tokenizer with {len(loaded_tokenizer)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "Now that you've trained a tokenizer for Tibetan, here's how you can use it with a Whisper model:\n",
    "\n",
    "```python\n",
    "from transformers import WhisperForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# Load your trained tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"data/whisper_tibetan_tokenizer\")\n",
    "\n",
    "# Load a Whisper model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# Resize the token embeddings to match your tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Now you can fine-tune or use the model with your tokenizer\n",
    "```\n",
    "\n",
    "Remember to always resize the token embeddings of the model after loading your custom tokenizer!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
