{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Fast Whisper Tokenizer for Tibetan\n",
    "\n",
    "This notebook demonstrates how to train a Whisper tokenizer for Tibetan text using the `PreTrainedTokenizerFast` class, which correctly supports the `train_new_from_iterator` method.\n",
    "\n",
    "We'll follow these steps:\n",
    "1. Load a fast tokenizer from the Whisper model\n",
    "2. Prepare a Tibetan text corpus\n",
    "3. Train the tokenizer on the Tibetan corpus\n",
    "4. Test and evaluate the tokenizer\n",
    "5. Save the trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/gangagyatso/.local/lib/python3.10/site-packages (4.56.1)\n",
      "Requirement already satisfied: tokenizers in /home/gangagyatso/.local/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: tqdm in /home/gangagyatso/.local/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/gangagyatso/.local/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/gangagyatso/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gangagyatso/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/gangagyatso/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/gangagyatso/.local/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gangagyatso/.local/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gangagyatso/.local/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gangagyatso/.local/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install transformers tokenizers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from transformers import PreTrainedTokenizerFast, WhisperProcessor\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Base Fast Tokenizer\n",
    "\n",
    "We'll load the tokenizer from the Whisper model using `PreTrainedTokenizerFast` which supports the `train_new_from_iterator` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading fast tokenizer from /home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe/bpe_tokenizer.json...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Calling PreTrainedTokenizerFast.from_pretrained() with the path to a single file or url is not supported for this tokenizer. Use a model identifier or the path to a directory instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the fast tokenizer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Loading fast tokenizer from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_MODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPreTrainedTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Fast tokenizer loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Check vocabulary size\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1934\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(pretrained_model_name_or_path) \u001b[38;5;129;01mor\u001b[39;00m is_remote_url(pretrained_model_name_or_path):\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_files_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 1934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1935\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_pretrained() with the path to a single file or url is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1936\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported for this tokenizer. Use a model identifier or the path to a directory instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1937\u001b[0m         )\n\u001b[1;32m   1938\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_pretrained() with the path to a single file or url is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1940\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be possible anymore in v5. Use a model identifier or the path to a directory instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1941\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1942\u001b[0m     )\n\u001b[1;32m   1943\u001b[0m     file_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_files_names\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Calling PreTrainedTokenizerFast.from_pretrained() with the path to a single file or url is not supported for this tokenizer. Use a model identifier or the path to a directory instead."
     ]
    }
   ],
   "source": [
    "# Define the base model\n",
    "BASE_MODEL = \"/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe/bpe_tokenizer.json\"\n",
    "\n",
    "# Load the fast tokenizer\n",
    "print(f\"🚀 Loading fast tokenizer from {BASE_MODEL}...\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(BASE_MODEL)\n",
    "print(\"✅ Fast tokenizer loaded successfully\")\n",
    "\n",
    "# Check vocabulary size\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output directory\n",
    "OUTPUT_DIR = \"data/whisper_tokenizer\"\n",
    "\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Trained tokenizer saved to: data/whisper_tokenizer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"💾 Trained tokenizer saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded tokenizer with 51865 tokens\n"
     ]
    }
   ],
   "source": [
    "# Test loading it back\n",
    "loaded_tokenizer = PreTrainedTokenizerFast.from_pretrained(OUTPUT_DIR)\n",
    "print(f\"✅ Successfully loaded tokenizer with {len(loaded_tokenizer)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the Tibetan Corpus\n",
    "\n",
    "Create an iterator for the Tibetan corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found corpus file: data/corpus/mergedcorpus.txt\n",
      "\n",
      "Sample of corpus:\n",
      "Line 1: ཨེ། སུའི་སྣང་བར་དེ་ཡོད་ན་\n",
      "Line 2: སེམས་ཅན་ཐམས་ཅད་ཀྱི་ཚབ་ལ་གཞན་བསམ་ཚར་དུས་བདག་འཛིན་ཡོད་མ་རེད་\n",
      "Line 3: རང་ཐོག་ལ་དངོས་སུ་སྙིང་རྗེ་ཡིས་འབྲེལ་བ་ཡོང་གི་ཡོད་རེད་\n"
     ]
    }
   ],
   "source": [
    "# Path to your Tibetan corpus file\n",
    "CORPUS_FILE = \"data/corpus/mergedcorpus.txt\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(CORPUS_FILE):\n",
    "    print(f\"❌ Corpus file not found: {CORPUS_FILE}\")\n",
    "else:\n",
    "    print(f\"✅ Found corpus file: {CORPUS_FILE}\")\n",
    "    \n",
    "    # Look at a sample of the corpus\n",
    "    with open(CORPUS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        sample_lines = [next(f).strip() for _ in range(3) if f]\n",
    "    \n",
    "    print(\"\\nSample of corpus:\")\n",
    "    for i, line in enumerate(sample_lines):\n",
    "        print(f\"Line {i+1}: {line[:100]}...\" if len(line) > 100 else f\"Line {i+1}: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define corpus iterator function\n",
    "def corpus_iterator():\n",
    "    with open(CORPUS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                yield line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Initial Tokenizer on Tibetan\n",
    "\n",
    "Let's first see how the original tokenizer handles Tibetan text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mབདེ་ལེགས་\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(sentence)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mids)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tokens'"
     ]
    }
   ],
   "source": [
    "# Test encoding a sentence\n",
    "sentence = \"བདེ་ལེགས་\"\n",
    "output = tokenizer.encode(sentence)\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"IDs:\", output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: [50258, 50363, 156, 121, 244, 156, 121, 239, 156, 121, 118, 156, 120, 233, 156, 121, 96, 156, 121, 118, 156, 121, 224, 156, 121, 99, 156, 120, 233, 50257]\n",
      "Decoded Text: <|startoftranscript|><|notimestamps|>བདེ་ལེགས་<|endoftext|>\n",
      "token len:  30\n"
     ]
    }
   ],
   "source": [
    "# The list of token IDs you want to decode\n",
    "ids_to_decode = [50258, 50363, 156, 121, 244, 156, 121, 239, 156, 121, 118, 156, 120, 233, 156, 121, 96, 156, 121, 118, 156, 121, 224, 156, 121, 99, 156, 120, 233, 50257]\n",
    "\n",
    "# Use the decode method to convert IDs back to text\n",
    "decoded_text = loaded_tokenizer.decode(ids_to_decode)\n",
    "\n",
    "print(f\"IDs: {ids_to_decode}\")\n",
    "print(f\"Decoded Text: {decoded_text}\")\n",
    "print(\"token len: \", len(ids_to_decode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokenizer results:\n",
      "Text: བྱང་ཆུབ་ཀྱི་སེམས་རྣམ་པ་གཉིས་ཡོད་རེད།\n",
      "Token count: 108\n",
      "Tokens: ['à', '½', 'ĸ', 'à', '¾', '±', 'à', '½', 'Ħ', 'à', '¼', 'ĭ', 'à', '½', 'Ĩ', 'à', '½', '´', 'à', '½', 'ĸ', 'à', '¼', 'ĭ', 'à', '½', 'Ģ', 'à', '¾', '±', 'à', '½', '²', 'à', '¼', 'ĭ', 'à', '½', '¦', 'à', '½', 'º', 'à', '½', 'ĺ', 'à', '½', '¦', 'à', '¼', 'ĭ', 'à', '½', '¢', 'à', '¾', '£', 'à', '½', 'ĺ', 'à', '¼', 'ĭ', 'à', '½', 'Ķ', 'à', '¼', 'ĭ', 'à', '½', 'Ĥ', 'à', '½', 'ī', 'à', '½', '²', 'à', '½', '¦', 'à', '¼', 'ĭ', 'à', '½', '¡', 'à', '½', '¼', 'à', '½', 'ĳ', 'à', '¼', 'ĭ', 'à', '½', '¢', 'à', '½', 'º', 'à', '½', 'ĳ', 'à', '¼', 'į']\n",
      "Token IDs: [50258, 50363, 156, 121, 244, 156, 122, 109, 156, 121, 226, 156, 120, 233, 156, 121, 228, 156, 121, 112, 156, 121, 244, 156, 120, 233, 156, 121, 222, 156, 122, 109, 156, 121, 110, 156, 120, 233, 156, 121, 99, 156, 121, 118, 156, 121, 246, 156, 121, 99, 156, 120, 233, 156, 121, 95, 156, 122, 96, 156, 121, 246, 156, 120, 233, 156, 121, 242, 156, 120, 233, 156, 121, 224, 156, 121, 231, 156, 121, 110, 156, 121, 99, 156, 120, 233, 156, 121, 94, 156, 121, 120, 156, 121, 239, 156, 120, 233, 156, 121, 95, 156, 121, 118, 156, 121, 239, 156, 120, 235, 50257]\n",
      "\n",
      "Roundtrip: True\n",
      "Decoded text: <|startoftranscript|><|notimestamps|>བྱང་ཆུབ་ཀྱི་སེམས་རྣམ་པ་གཉིས་ཡོད་རེད།<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Example Tibetan text\n",
    "tibetan_example = \"བྱང་ཆུབ་ཀྱི་སེམས་རྣམ་པ་གཉིས་ཡོད་རེད།\"\n",
    "\n",
    "# Tokenize with the original tokenizer\n",
    "tokens = tokenizer.tokenize(tibetan_example)\n",
    "token_ids = tokenizer.encode(tibetan_example)\n",
    "\n",
    "print(\"Original tokenizer results:\")\n",
    "print(f\"Text: {tibetan_example}\")\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Check roundtrip\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"\\nRoundtrip: {tibetan_example in decoded}\")\n",
    "print(f\"Decoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Tokenizer on Tibetan Corpus\n",
    "\n",
    "Now we'll train the tokenizer on our Tibetan corpus to improve its handling of Tibetan text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Original vocabulary size: 51865\n",
      "🎯 Target vocabulary size: 61865\n",
      "⏳ Starting tokenizer training... This may take a few minutes.\n",
      "\n",
      "\n",
      "\n",
      "✅ Training complete!\n",
      "📈 New vocabulary size: 3059\n"
     ]
    }
   ],
   "source": [
    "# Set target vocabulary size\n",
    "TARGET_VOCAB_SIZE = len(tokenizer) + 10000\n",
    "\n",
    "print(f\"🧠 Original vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"🎯 Target vocabulary size: {TARGET_VOCAB_SIZE}\")\n",
    "\n",
    "print(\"⏳ Starting tokenizer training... This may take a few minutes.\")\n",
    "\n",
    "# Train the tokenizer\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(corpus_iterator(), vocab_size=TARGET_VOCAB_SIZE, )\n",
    "\n",
    "print(\"✅ Training complete!\")\n",
    "print(f\"📈 New vocabulary size: {len(new_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Trained Tokenizer\n",
    "\n",
    "Let's check how the newly trained tokenizer handles Tibetan text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test examples\n",
    "test_examples = [\n",
    "    \"བྱང་ཆུབ་ཀྱི་སེམས་རྣམ་པ་གཉིས་ཡོད་རེད།\",  # Simple sentence\n",
    "    \"བོད་ཀྱི་སྐད་ཡིག་ནི་ལོ་རྒྱུས་ཧ་ཅང་རིང་པོ་ཡོད་པའི་སྐད་ཡིག་ཅིག་རེད།\",  # Another example\n",
    "    \"བྱང་ཆུབ་ཀྱི་སེམས་རྣམ་པ་གཉིས་ཡོད་རེད། བྱང་ཆུབ་ཀྱི་སེམས་ཡོད་ན་ཡེ་ཤེས་ལྷ་ལ་འགྱུར་འགྲོ་གི་ཡོད་རེད། བྱང་ཆུབ་ཀྱི་སེམས་མེད་ན། དེ་ནས་འདི་\"\n",
    "]\n",
    "\n",
    "# Function to analyze tokenization\n",
    "def analyze_tokenization(tokenizer, text, name=\"Tokenizer\"):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    decoded = tokenizer.decode(token_ids)\n",
    "    \n",
    "    print(f\"\\n--- {name} Results ---\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Token count: {len(tokens)}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Roundtrip successful: {text in decoded}\")\n",
    "    \n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Test Example 1 ====\n",
      "\n",
      "\n",
      "--- Original Tokenizer Results ---\n",
      "Text: བྱང་ཆུབ་ཀྱི་སེམས་རྣམ་པ་གཉིས་ཡོད་རེད།\n",
      "Token count: 108\n",
      "Tokens: ['à', '½', 'ĸ', 'à', '¾', '±', 'à', '½', 'Ħ', 'à', '¼', 'ĭ', 'à', '½', 'Ĩ', 'à', '½', '´', 'à', '½', 'ĸ', 'à', '¼', 'ĭ', 'à', '½', 'Ģ', 'à', '¾', '±', 'à', '½', '²', 'à', '¼', 'ĭ', 'à', '½', '¦', 'à', '½', 'º', 'à', '½', 'ĺ', 'à', '½', '¦', 'à', '¼', 'ĭ', 'à', '½', '¢', 'à', '¾', '£', 'à', '½', 'ĺ', 'à', '¼', 'ĭ', 'à', '½', 'Ķ', 'à', '¼', 'ĭ', 'à', '½', 'Ĥ', 'à', '½', 'ī', 'à', '½', '²', 'à', '½', '¦', 'à', '¼', 'ĭ', 'à', '½', '¡', 'à', '½', '¼', 'à', '½', 'ĳ', 'à', '¼', 'ĭ', 'à', '½', '¢', 'à', '½', 'º', 'à', '½', 'ĳ', 'à', '¼', 'į']\n",
      "Roundtrip successful: True\n",
      "\n",
      "--- Trained Tokenizer Results ---\n",
      "Text: བྱང་ཆུབ་ཀྱི་སེམས་རྣམ་པ་གཉིས་ཡོད་རེད།\n",
      "Token count: 32\n",
      "Tokens: ['à½ĸ', 'à¾±', 'à½Ħ', 'à¼ĭ', 'à½Ĩ', 'à½´', 'à½ĸ', 'à¼ĭ', 'à½Ģ', 'à¾±à½²à¼ĭ', 'à½¦', 'à½º', 'à½ĺà½¦', 'à¼ĭ', 'à½¢', 'à¾£', 'à½ĺ', 'à¼ĭ', 'à½Ķ', 'à¼ĭ', 'à½Ĥà½ī', 'à½²', 'à½¦', 'à¼ĭ', 'à½¡', 'à½¼', 'à½ĳ', 'à¼ĭ', 'à½¢', 'à½º', 'à½ĳ', 'à¼į']\n",
      "Roundtrip successful: True\n",
      "\n",
      "✨ Improvement: 76 fewer tokens used!\n",
      "\n",
      "==== Test Example 2 ====\n",
      "\n",
      "\n",
      "--- Original Tokenizer Results ---\n",
      "Text: བོད་ཀྱི་སྐད་ཡིག་ནི་ལོ་རྒྱུས་ཧ་ཅང་རིང་པོ་ཡོད་པའི་སྐད་ཡིག་ཅིག་རེད།\n",
      "Token count: 192\n",
      "Tokens: ['à', '½', 'ĸ', 'à', '½', '¼', 'à', '½', 'ĳ', 'à', '¼', 'ĭ', 'à', '½', 'Ģ', 'à', '¾', '±', 'à', '½', '²', 'à', '¼', 'ĭ', 'à', '½', '¦', 'à', '¾', 'Ĳ', 'à', '½', 'ĳ', 'à', '¼', 'ĭ', 'à', '½', '¡', 'à', '½', '²', 'à', '½', 'Ĥ', 'à', '¼', 'ĭ', 'à', '½', 'ĵ', 'à', '½', '²', 'à', '¼', 'ĭ', 'à', '½', '£', 'à', '½', '¼', 'à', '¼', 'ĭ', 'à', '½', '¢', 'à', '¾', 'Ĵ', 'à', '¾', '±', 'à', '½', '´', 'à', '½', '¦', 'à', '¼', 'ĭ', 'à', '½', '§', 'à', '¼', 'ĭ', 'à', '½', 'ħ', 'à', '½', 'Ħ', 'à', '¼', 'ĭ', 'à', '½', '¢', 'à', '½', '²', 'à', '½', 'Ħ', 'à', '¼', 'ĭ', 'à', '½', 'Ķ', 'à', '½', '¼', 'à', '¼', 'ĭ', 'à', '½', '¡', 'à', '½', '¼', 'à', '½', 'ĳ', 'à', '¼', 'ĭ', 'à', '½', 'Ķ', 'à', '½', 'ł', 'à', '½', '²', 'à', '¼', 'ĭ', 'à', '½', '¦', 'à', '¾', 'Ĳ', 'à', '½', 'ĳ', 'à', '¼', 'ĭ', 'à', '½', '¡', 'à', '½', '²', 'à', '½', 'Ĥ', 'à', '¼', 'ĭ', 'à', '½', 'ħ', 'à', '½', '²', 'à', '½', 'Ĥ', 'à', '¼', 'ĭ', 'à', '½', '¢', 'à', '½', 'º', 'à', '½', 'ĳ', 'à', '¼', 'į']\n",
      "Roundtrip successful: True\n",
      "\n",
      "--- Trained Tokenizer Results ---\n",
      "Text: བོད་ཀྱི་སྐད་ཡིག་ནི་ལོ་རྒྱུས་ཧ་ཅང་རིང་པོ་ཡོད་པའི་སྐད་ཡིག་ཅིག་རེད།\n",
      "Token count: 54\n",
      "Tokens: ['à½ĸ', 'à½¼', 'à½ĳ', 'à¼ĭ', 'à½Ģ', 'à¾±à½²à¼ĭ', 'à½¦', 'à¾Ĳ', 'à½ĳ', 'à¼ĭ', 'à½¡', 'à½²', 'à½Ĥ', 'à¼ĭ', 'à½ĵ', 'à½²à¼ĭ', 'à½£', 'à½¼à¼ĭ', 'à½¢', 'à¾Ĵà¾±à½´', 'à½¦', 'à¼ĭ', 'à½§', 'à¼ĭ', 'à½ħà½Ħ', 'à¼ĭ', 'à½¢', 'à½²', 'à½Ħ', 'à¼ĭ', 'à½Ķ', 'à½¼à¼ĭ', 'à½¡', 'à½¼', 'à½ĳ', 'à¼ĭ', 'à½Ķà½ł', 'à½²à¼ĭ', 'à½¦', 'à¾Ĳ', 'à½ĳ', 'à¼ĭ', 'à½¡', 'à½²', 'à½Ĥ', 'à¼ĭ', 'à½ħ', 'à½²', 'à½Ĥ', 'à¼ĭ', 'à½¢', 'à½º', 'à½ĳ', 'à¼į']\n",
      "Roundtrip successful: True\n",
      "\n",
      "✨ Improvement: 138 fewer tokens used!\n",
      "\n",
      "==== Test Example 3 ====\n",
      "\n",
      "\n",
      "--- Original Tokenizer Results ---\n",
      "Text: བྱང་ཆུབ་ཀྱི་སེམས་རྣམ་པ་གཉིས་ཡོད་རེད། བྱང་ཆུབ་ཀྱི་སེམས་ཡོད་ན་ཡེ་ཤེས་ལྷ་ལ་འགྱུར་འགྲོ་གི་ཡོད་རེད། བྱང་ཆུབ་ཀྱི་སེམས་མེད་ན། དེ་ནས་འདི་\n",
      "Token count: 381\n",
      "Tokens: ['à', '½', 'ĸ', 'à', '¾', '±', 'à', '½', 'Ħ', 'à', '¼', 'ĭ', 'à', '½', 'Ĩ', 'à', '½', '´', 'à', '½', 'ĸ', 'à', '¼', 'ĭ', 'à', '½', 'Ģ', 'à', '¾', '±', 'à', '½', '²', 'à', '¼', 'ĭ', 'à', '½', '¦', 'à', '½', 'º', 'à', '½', 'ĺ', 'à', '½', '¦', 'à', '¼', 'ĭ', 'à', '½', '¢', 'à', '¾', '£', 'à', '½', 'ĺ', 'à', '¼', 'ĭ', 'à', '½', 'Ķ', 'à', '¼', 'ĭ', 'à', '½', 'Ĥ', 'à', '½', 'ī', 'à', '½', '²', 'à', '½', '¦', 'à', '¼', 'ĭ', 'à', '½', '¡', 'à', '½', '¼', 'à', '½', 'ĳ', 'à', '¼', 'ĭ', 'à', '½', '¢', 'à', '½', 'º', 'à', '½', 'ĳ', 'à', '¼', 'į', 'Ġ', 'à', '½', 'ĸ', 'à', '¾', '±', 'à', '½', 'Ħ', 'à', '¼', 'ĭ', 'à', '½', 'Ĩ', 'à', '½', '´', 'à', '½', 'ĸ', 'à', '¼', 'ĭ', 'à', '½', 'Ģ', 'à', '¾', '±', 'à', '½', '²', 'à', '¼', 'ĭ', 'à', '½', '¦', 'à', '½', 'º', 'à', '½', 'ĺ', 'à', '½', '¦', 'à', '¼', 'ĭ', 'à', '½', '¡', 'à', '½', '¼', 'à', '½', 'ĳ', 'à', '¼', 'ĭ', 'à', '½', 'ĵ', 'à', '¼', 'ĭ', 'à', '½', '¡', 'à', '½', 'º', 'à', '¼', 'ĭ', 'à', '½', '¤', 'à', '½', 'º', 'à', '½', '¦', 'à', '¼', 'ĭ', 'à', '½', '£', 'à', '¾', '·', 'à', '¼', 'ĭ', 'à', '½', '£', 'à', '¼', 'ĭ', 'à', '½', 'ł', 'à', '½', 'Ĥ', 'à', '¾', '±', 'à', '½', '´', 'à', '½', '¢', 'à', '¼', 'ĭ', 'à', '½', 'ł', 'à', '½', 'Ĥ', 'à', '¾', '²', 'à', '½', '¼', 'à', '¼', 'ĭ', 'à', '½', 'Ĥ', 'à', '½', '²', 'à', '¼', 'ĭ', 'à', '½', '¡', 'à', '½', '¼', 'à', '½', 'ĳ', 'à', '¼', 'ĭ', 'à', '½', '¢', 'à', '½', 'º', 'à', '½', 'ĳ', 'à', '¼', 'į', 'Ġ', 'à', '½', 'ĸ', 'à', '¾', '±', 'à', '½', 'Ħ', 'à', '¼', 'ĭ', 'à', '½', 'Ĩ', 'à', '½', '´', 'à', '½', 'ĸ', 'à', '¼', 'ĭ', 'à', '½', 'Ģ', 'à', '¾', '±', 'à', '½', '²', 'à', '¼', 'ĭ', 'à', '½', '¦', 'à', '½', 'º', 'à', '½', 'ĺ', 'à', '½', '¦', 'à', '¼', 'ĭ', 'à', '½', 'ĺ', 'à', '½', 'º', 'à', '½', 'ĳ', 'à', '¼', 'ĭ', 'à', '½', 'ĵ', 'à', '¼', 'į', 'Ġ', 'à', '½', 'ĳ', 'à', '½', 'º', 'à', '¼', 'ĭ', 'à', '½', 'ĵ', 'à', '½', '¦', 'à', '¼', 'ĭ', 'à', '½', 'ł', 'à', '½', 'ĳ', 'à', '½', '²', 'à', '¼', 'ĭ']\n",
      "Roundtrip successful: True\n",
      "\n",
      "--- Trained Tokenizer Results ---\n",
      "Text: བྱང་ཆུབ་ཀྱི་སེམས་རྣམ་པ་གཉིས་ཡོད་རེད། བྱང་ཆུབ་ཀྱི་སེམས་ཡོད་ན་ཡེ་ཤེས་ལྷ་ལ་འགྱུར་འགྲོ་གི་ཡོད་རེད། བྱང་ཆུབ་ཀྱི་སེམས་མེད་ན། དེ་ནས་འདི་\n",
      "Token count: 104\n",
      "Tokens: ['à½ĸ', 'à¾±', 'à½Ħ', 'à¼ĭ', 'à½Ĩ', 'à½´', 'à½ĸ', 'à¼ĭ', 'à½Ģ', 'à¾±à½²à¼ĭ', 'à½¦', 'à½º', 'à½ĺà½¦', 'à¼ĭ', 'à½¢', 'à¾£', 'à½ĺ', 'à¼ĭ', 'à½Ķ', 'à¼ĭ', 'à½Ĥà½ī', 'à½²', 'à½¦', 'à¼ĭ', 'à½¡', 'à½¼', 'à½ĳ', 'à¼ĭ', 'à½¢', 'à½º', 'à½ĳ', 'à¼į', 'Ġà½ĸ', 'à¾±', 'à½Ħ', 'à¼ĭ', 'à½Ĩ', 'à½´', 'à½ĸ', 'à¼ĭ', 'à½Ģ', 'à¾±à½²à¼ĭ', 'à½¦', 'à½º', 'à½ĺà½¦', 'à¼ĭ', 'à½¡', 'à½¼', 'à½ĳ', 'à¼ĭ', 'à½ĵ', 'à¼ĭ', 'à½¡', 'à½ºà¼ĭ', 'à½¤', 'à½º', 'à½¦', 'à¼ĭ', 'à½£', 'à¾·à¼ĭ', 'à½£', 'à¼ĭ', 'à½łà½Ĥ', 'à¾±à½´', 'à½¢', 'à¼ĭ', 'à½łà½Ĥ', 'à¾²à½¼à¼ĭ', 'à½Ĥ', 'à½²à¼ĭ', 'à½¡', 'à½¼', 'à½ĳ', 'à¼ĭ', 'à½¢', 'à½º', 'à½ĳ', 'à¼į', 'Ġà½ĸ', 'à¾±', 'à½Ħ', 'à¼ĭ', 'à½Ĩ', 'à½´', 'à½ĸ', 'à¼ĭ', 'à½Ģ', 'à¾±à½²à¼ĭ', 'à½¦', 'à½º', 'à½ĺà½¦', 'à¼ĭ', 'à½ĺ', 'à½º', 'à½ĳ', 'à¼ĭ', 'à½ĵ', 'à¼į', 'Ġà½ĳ', 'à½ºà¼ĭ', 'à½ĵà½¦', 'à¼ĭ', 'à½łà½ĳ', 'à½²à¼ĭ']\n",
      "Roundtrip successful: True\n",
      "\n",
      "✨ Improvement: 277 fewer tokens used!\n"
     ]
    }
   ],
   "source": [
    "# Compare tokenization between original and new tokenizer\n",
    "for i, example in enumerate(test_examples):\n",
    "    print(f\"\\n==== Test Example {i+1} ====\\n\")\n",
    "    \n",
    "    # Test original tokenizer\n",
    "    orig_count = analyze_tokenization(tokenizer, example, \"Original Tokenizer\")\n",
    "    \n",
    "    # Test new tokenizer\n",
    "    new_count = analyze_tokenization(new_tokenizer, example, \"Trained Tokenizer\")\n",
    "    \n",
    "    # Compare\n",
    "    if new_count < orig_count:\n",
    "        print(f\"\\n✨ Improvement: {orig_count - new_count} fewer tokens used!\")\n",
    "    elif new_count == orig_count:\n",
    "        print(f\"\\n🔄 Same number of tokens used.\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ New tokenizer uses {new_count - orig_count} more tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze the New Vocabulary\n",
    "\n",
    "Let's analyze what new tokens were added and check for Tibetan-specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2695 new tokens to the vocabulary\n",
      "Added 0 new Tibetan tokens\n"
     ]
    }
   ],
   "source": [
    "# Get vocabularies\n",
    "old_vocab = tokenizer.get_vocab()\n",
    "new_vocab = new_tokenizer.get_vocab()\n",
    "\n",
    "# Find new tokens\n",
    "new_tokens = [token for token in new_vocab.keys() if token not in old_vocab]\n",
    "print(f\"Added {len(new_tokens)} new tokens to the vocabulary\")\n",
    "\n",
    "# Find Tibetan tokens\n",
    "tibetan_range = (0x0F00, 0x0FFF)  # Unicode range for Tibetan\n",
    "new_tibetan_tokens = [token for token in new_tokens \n",
    "                      if any(ord(c) >= tibetan_range[0] and ord(c) <= tibetan_range[1] for c in token)]\n",
    "\n",
    "print(f\"Added {len(new_tibetan_tokens)} new Tibetan tokens\")\n",
    "\n",
    "# Show some examples\n",
    "if new_tibetan_tokens:\n",
    "    print(\"\\nSample new Tibetan tokens:\")\n",
    "    for token in new_tibetan_tokens[:20]:  # Show up to 20\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Trained Tokenizer\n",
    "\n",
    "Save the new tokenizer for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Trained tokenizer saved to: data/whisper_latin_tibetan_tokenizer\n",
      "✅ Successfully loaded tokenizer with 3059 tokens\n"
     ]
    }
   ],
   "source": [
    "# Define output directory\n",
    "OUTPUT_DIR = \"data/whisper_latin_tibetan_tokenizer\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the tokenizer\n",
    "new_tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"💾 Trained tokenizer saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Test loading it back\n",
    "loaded_tokenizer = PreTrainedTokenizerFast.from_pretrained(OUTPUT_DIR)\n",
    "print(f\"✅ Successfully loaded tokenizer with {len(loaded_tokenizer)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "Now that you've trained a tokenizer for Tibetan, here's how you can use it with a Whisper model:\n",
    "\n",
    "```python\n",
    "from transformers import WhisperForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# Load your trained tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"data/whisper_tibetan_tokenizer\")\n",
    "\n",
    "# Load a Whisper model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# Resize the token embeddings to match your tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Now you can fine-tune or use the model with your tokenizer\n",
    "```\n",
    "\n",
    "Remember to always resize the token embeddings of the model after loading your custom tokenizer!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
