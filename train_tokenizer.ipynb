{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63a3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "corpus_file = \"/home/gangagyatso/Desktop/stt-bpe-trainer/data/corpus/mergedcorpus.txt\"   # your input text file (one transcript per line)\n",
    "output_dir = \"/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe\"          # where to save tokenizer files\n",
    "vocab_size = 10000                  # adjust (8k‚Äì15k typical for Tibetan ASR)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f5a7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created a sample corpus file at: /home/gangagyatso/Desktop/stt-bpe-trainer/data/corpus/mergedcorpus.txt\n",
      "üöÄ Starting tokenizer training...\n",
      "\n",
      "\n",
      "\n",
      "‚úÖ Training complete!\n",
      "üíæ Tokenizer saved to: /home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe/bpe_tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# train_bpe_tokenizer.py\n",
    "#\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Sequence, Whitespace, CharDelimiterSplit, Split\n",
    "\n",
    "print(f\"‚úÖ Created a sample corpus file at: {corpus_file}\")\n",
    "\n",
    "files = [corpus_file]\n",
    "\n",
    "# --- 2. Initialize the Tokenizer ---\n",
    "# We start with an empty BPE model.\n",
    "# The <unk> token is used for out-of-vocabulary words.\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<|endoftext|>\"))\n",
    "\n",
    "# --- 3. Configure the Trainer ---\n",
    "# The BpeTrainer will learn the merge rules from our data.\n",
    "# vocab_size: The total number of tokens we want in our vocabulary.\n",
    "# special_tokens: A list of tokens that have special meaning for the model.\n",
    "trainer = BpeTrainer(vocab_size=vocab_size, min_frequency=1000)\n",
    "\n",
    "# --- 4. Set a Pre-tokenizer ---\n",
    "# The pre-tokenizer splits the input text into initial words.\n",
    "# Splitting by whitespace is a common first step.\n",
    "\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()  # keep syllable+tsheg intact\n",
    "#tokenizer.pre_tokenizer = Sequence([Whitespace(), CharDelimiterSplit(\"‡ºã\")])\n",
    "# Pre-tokenizer: whitespace + tsheg + shad\n",
    "#tokenizer.pre_tokenizer = Sequence([\n",
    "#    Whitespace(),\n",
    "#    Split(\"‡ºã\", behavior=\"isolated\"),   # keep tsheg as its own token\n",
    "#    Split(\"‡ºç\", behavior=\"isolated\")    # keep shad as its own token\n",
    "#])\n",
    "# This is the corrected pre-tokenizer from our previous discussion to merge \"‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã\"\n",
    "#tokenizer.pre_tokenizer = Sequence([\n",
    "#    Whitespace(), \n",
    "#    Split('‡ºã', behavior='merged_with_previous')\n",
    "#])\n",
    "# --- 5. Train the Tokenizer ---\n",
    "print(\"üöÄ Starting tokenizer training...\")\n",
    "tokenizer.train(files, trainer)\n",
    "print(\"‚úÖ Training complete!\")\n",
    "\n",
    "# --- 6. Save the Tokenizer ---\n",
    "# The trained tokenizer is saved to a single JSON file.\n",
    "output_path = \"/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe/bpe_tokenizer.json\"\n",
    "tokenizer.save(output_path)\n",
    "print(f\"üíæ Tokenizer saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20027b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['‡Ωñ‡Ωë‡Ω∫', '‡ºã', '‡Ω£‡Ω∫‡ΩÇ‡Ω¶', '‡ºã']\n",
      "IDs: [234, 4, 746, 4]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Load full tokenizer config\n",
    "tokenizer = Tokenizer.from_file(\"/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe/bpe_tokenizer.json\")\n",
    "\n",
    "# Test it\n",
    "output = tokenizer.encode(\"‡Ωñ‡Ωë‡Ω∫‡ºã‡Ω£‡Ω∫‡ΩÇ‡Ω¶‡ºã\")\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"IDs:\", output.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4939280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing the new tokenizer ---\n",
      "Original sentence: ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω¢‡æ£‡Ωò‡ºã‡Ωî‡ºã‡ΩÇ‡Ωâ‡Ω≤‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ωì‡ºã‡Ω°‡Ω∫‡ºã‡Ω§‡Ω∫‡Ω¶‡ºã‡Ω£‡æ∑‡ºã‡Ω£‡ºã‡Ω†‡ΩÇ‡æ±‡Ω¥‡Ω¢‡ºã‡Ω†‡ΩÇ‡æ≤‡Ωº‡ºã‡ΩÇ‡Ω≤‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ωò‡Ω∫‡Ωë‡ºã‡Ωì‡ºç ‡Ωë‡Ω∫‡ºã‡Ωì‡Ω¶‡ºã‡Ω†‡Ωë‡Ω≤‡ºã\n",
      "Encoded tokens (IDs): [272, 4, 299, 4, 137, 4, 148, 4, 248, 4, 31, 4, 227, 4, 117, 4, 109, 6, 272, 4, 299, 4, 137, 4, 148, 4, 117, 4, 30, 4, 315, 4, 168, 4, 256, 4, 45, 4, 384, 4, 191, 4, 116, 4, 117, 4, 109, 6, 272, 4, 299, 4, 137, 4, 148, 4, 156, 4, 30, 6, 105, 4, 107, 4, 150, 4]\n",
      "Encoded tokens (strings): ['‡Ωñ‡æ±‡ΩÑ', '‡ºã', '‡ΩÜ‡Ω¥‡Ωñ', '‡ºã', '‡ΩÄ‡æ±‡Ω≤', '‡ºã', '‡Ω¶‡Ω∫‡Ωò‡Ω¶', '‡ºã', '‡Ω¢‡æ£‡Ωò', '‡ºã', '‡Ωî', '‡ºã', '‡ΩÇ‡Ωâ‡Ω≤‡Ω¶', '‡ºã', '‡Ω°‡Ωº‡Ωë', '‡ºã', '‡Ω¢‡Ω∫‡Ωë', '‡ºç', '‡Ωñ‡æ±‡ΩÑ', '‡ºã', '‡ΩÜ‡Ω¥‡Ωñ', '‡ºã', '‡ΩÄ‡æ±‡Ω≤', '‡ºã', '‡Ω¶‡Ω∫‡Ωò‡Ω¶', '‡ºã', '‡Ω°‡Ωº‡Ωë', '‡ºã', '‡Ωì', '‡ºã', '‡Ω°‡Ω∫', '‡ºã', '‡Ω§‡Ω∫‡Ω¶', '‡ºã', '‡Ω£‡æ∑', '‡ºã', '‡Ω£', '‡ºã', '‡Ω†‡ΩÇ‡æ±‡Ω¥‡Ω¢', '‡ºã', '‡Ω†‡ΩÇ‡æ≤‡Ωº', '‡ºã', '‡ΩÇ‡Ω≤', '‡ºã', '‡Ω°‡Ωº‡Ωë', '‡ºã', '‡Ω¢‡Ω∫‡Ωë', '‡ºç', '‡Ωñ‡æ±‡ΩÑ', '‡ºã', '‡ΩÜ‡Ω¥‡Ωñ', '‡ºã', '‡ΩÄ‡æ±‡Ω≤', '‡ºã', '‡Ω¶‡Ω∫‡Ωò‡Ω¶', '‡ºã', '‡Ωò‡Ω∫‡Ωë', '‡ºã', '‡Ωì', '‡ºç', '‡Ωë‡Ω∫', '‡ºã', '‡Ωì‡Ω¶', '‡ºã', '‡Ω†‡Ωë‡Ω≤', '‡ºã']\n",
      "Decoded tokens: ‡Ωñ‡æ±‡ΩÑ ‡ºã ‡ΩÜ‡Ω¥‡Ωñ ‡ºã ‡ΩÄ‡æ±‡Ω≤ ‡ºã ‡Ω¶‡Ω∫‡Ωò‡Ω¶ ‡ºã ‡Ω¢‡æ£‡Ωò ‡ºã ‡Ωî ‡ºã ‡ΩÇ‡Ωâ‡Ω≤‡Ω¶ ‡ºã ‡Ω°‡Ωº‡Ωë ‡ºã ‡Ω¢‡Ω∫‡Ωë ‡ºç ‡Ωñ‡æ±‡ΩÑ ‡ºã ‡ΩÜ‡Ω¥‡Ωñ ‡ºã ‡ΩÄ‡æ±‡Ω≤ ‡ºã ‡Ω¶‡Ω∫‡Ωò‡Ω¶ ‡ºã ‡Ω°‡Ωº‡Ωë ‡ºã ‡Ωì ‡ºã ‡Ω°‡Ω∫ ‡ºã ‡Ω§‡Ω∫‡Ω¶ ‡ºã ‡Ω£‡æ∑ ‡ºã ‡Ω£ ‡ºã ‡Ω†‡ΩÇ‡æ±‡Ω¥‡Ω¢ ‡ºã ‡Ω†‡ΩÇ‡æ≤‡Ωº ‡ºã ‡ΩÇ‡Ω≤ ‡ºã ‡Ω°‡Ωº‡Ωë ‡ºã ‡Ω¢‡Ω∫‡Ωë ‡ºç ‡Ωñ‡æ±‡ΩÑ ‡ºã ‡ΩÜ‡Ω¥‡Ωñ ‡ºã ‡ΩÄ‡æ±‡Ω≤ ‡ºã ‡Ω¶‡Ω∫‡Ωò‡Ω¶ ‡ºã ‡Ωò‡Ω∫‡Ωë ‡ºã ‡Ωì ‡ºç ‡Ωë‡Ω∫ ‡ºã ‡Ωì‡Ω¶ ‡ºã ‡Ω†‡Ωë‡Ω≤ ‡ºã\n",
      "token len: 66\n",
      "\n",
      "Original sentence: ‡Ωñ‡æ≥‡ºã‡Ωò‡ºã‡Ωë‡ΩÇ‡Ω∫‡ºã‡Ωñ‡Ω†‡Ω≤‡ºã‡Ωñ‡Ω§‡Ω∫‡Ω¶‡ºã‡ΩÇ‡Ωâ‡Ω∫‡Ωì‡ºã‡Ω¶‡æê‡Ω¥‡ºã‡Ωë‡ΩÇ‡Ω∫‡ºã‡Ω†‡Ωë‡Ω¥‡Ωì‡ºã‡ΩÇ‡ΩÖ‡Ω≤‡ΩÇ‡ºã‡Ω°‡Ω≤‡Ωì‡ºã‡Ωì‡ºç\n",
      "Encoded tokens (IDs): [170, 4, 35, 4, 273, 4, 193, 4, 550, 4, 448, 4, 301, 4, 273, 4, 699, 4, 153, 4, 144, 4, 30, 6]\n",
      "Encoded tokens (strings): ['‡Ωñ‡æ≥', '‡ºã', '‡Ωò', '‡ºã', '‡Ωë‡ΩÇ‡Ω∫', '‡ºã', '‡Ωñ‡Ω†‡Ω≤', '‡ºã', '‡Ωñ‡Ω§‡Ω∫‡Ω¶', '‡ºã', '‡ΩÇ‡Ωâ‡Ω∫‡Ωì', '‡ºã', '‡Ω¶‡æê‡Ω¥', '‡ºã', '‡Ωë‡ΩÇ‡Ω∫', '‡ºã', '‡Ω†‡Ωë‡Ω¥‡Ωì', '‡ºã', '‡ΩÇ‡ΩÖ‡Ω≤‡ΩÇ', '‡ºã', '‡Ω°‡Ω≤‡Ωì', '‡ºã', '‡Ωì', '‡ºç']\n",
      "Decoded tokens: ‡Ωñ‡æ≥ ‡ºã ‡Ωò ‡ºã ‡Ωë‡ΩÇ‡Ω∫ ‡ºã ‡Ωñ‡Ω†‡Ω≤ ‡ºã ‡Ωñ‡Ω§‡Ω∫‡Ω¶ ‡ºã ‡ΩÇ‡Ωâ‡Ω∫‡Ωì ‡ºã ‡Ω¶‡æê‡Ω¥ ‡ºã ‡Ωë‡ΩÇ‡Ω∫ ‡ºã ‡Ω†‡Ωë‡Ω¥‡Ωì ‡ºã ‡ΩÇ‡ΩÖ‡Ω≤‡ΩÇ ‡ºã ‡Ω°‡Ω≤‡Ωì ‡ºã ‡Ωì ‡ºç\n",
      "token len: 24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 7. Load and Test the Tokenizer ---\n",
    "print(\"\\n--- Testing the new tokenizer ---\")\n",
    "# Load the tokenizer from the saved file\n",
    "loaded_tokenizer = Tokenizer.from_file(\"data/tokenizer_bpe/bpe_tokenizer.json\")\n",
    "\n",
    "# Test encoding a sentence\n",
    "sentence = \"‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω¢‡æ£‡Ωò‡ºã‡Ωî‡ºã‡ΩÇ‡Ωâ‡Ω≤‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ωì‡ºã‡Ω°‡Ω∫‡ºã‡Ω§‡Ω∫‡Ω¶‡ºã‡Ω£‡æ∑‡ºã‡Ω£‡ºã‡Ω†‡ΩÇ‡æ±‡Ω¥‡Ω¢‡ºã‡Ω†‡ΩÇ‡æ≤‡Ωº‡ºã‡ΩÇ‡Ω≤‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ωò‡Ω∫‡Ωë‡ºã‡Ωì‡ºç ‡Ωë‡Ω∫‡ºã‡Ωì‡Ω¶‡ºã‡Ω†‡Ωë‡Ω≤‡ºã\"\n",
    "output = loaded_tokenizer.encode(sentence)\n",
    "\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(f\"Encoded tokens (IDs): {output.ids}\")\n",
    "print(f\"Encoded tokens (strings): {output.tokens}\")\n",
    "print(f\"Decoded tokens: {loaded_tokenizer.decode(output.ids)}\")\n",
    "print(f\"token len: {len(output.tokens)}\")\n",
    "# The BPE algorithm learned to merge \"low\" + \"er\" -> \"lower\" and \"new\" + \"er\" -> \"newer\"\n",
    "sentence_2 = \"‡Ωñ‡æ≥‡ºã‡Ωò‡ºã‡Ωë‡ΩÇ‡Ω∫‡ºã‡Ωñ‡Ω†‡Ω≤‡ºã‡Ωñ‡Ω§‡Ω∫‡Ω¶‡ºã‡ΩÇ‡Ωâ‡Ω∫‡Ωì‡ºã‡Ω¶‡æê‡Ω¥‡ºã‡Ωë‡ΩÇ‡Ω∫‡ºã‡Ω†‡Ωë‡Ω¥‡Ωì‡ºã‡ΩÇ‡ΩÖ‡Ω≤‡ΩÇ‡ºã‡Ω°‡Ω≤‡Ωì‡ºã‡Ωì‡ºç\"\n",
    "output_2 = loaded_tokenizer.encode(sentence_2)\n",
    "print(f\"\\nOriginal sentence: {sentence_2}\")\n",
    "print(f\"Encoded tokens (IDs): {output_2.ids}\")\n",
    "print(f\"Encoded tokens (strings): {output_2.tokens}\")\n",
    "print(f\"Decoded tokens: {loaded_tokenizer.decode(output_2.ids)}\")\n",
    "print(f\"token len: {len(output_2.tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea2efb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved vocab.json to /home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe//vocab.json\n",
      "‚úÖ Saved merges.txt to /home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe//merges.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Path to the saved tokenizer.json (from Script 1)\n",
    "tokenizer_path = \"/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe/bpe_tokenizer.json\"\n",
    "output_dir = \"/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# Extract vocab (token ‚Üí id mapping)\n",
    "vocab = tokenizer.get_vocab()\n",
    "# sort by id (important!)\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
    "\n",
    "with open(os.path.join(output_dir, \"vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({token: idx for token, idx in sorted_vocab}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved vocab.json to {output_dir}/vocab.json\")\n",
    "\n",
    "# Extract merges\n",
    "model = tokenizer.to_str()  # stringified JSON of the whole tokenizer\n",
    "model_json = json.loads(model)\n",
    "\n",
    "if \"model\" in model_json and \"merges\" in model_json[\"model\"]:\n",
    "    merges = model_json[\"model\"][\"merges\"]\n",
    "    with open(os.path.join(output_dir, \"merges.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"#version: 0.2\\n\")\n",
    "        for merge in merges:\n",
    "            f.write(\" \".join(merge) + \"\\n\")\n",
    "    print(f\"‚úÖ Saved merges.txt to {output_dir}/merges.txt\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No merges found in tokenizer.json (check if it's really BPE)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e711858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "üíæ vocab.json and merges.txt written to ./tokenizer_bpe/\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import CharBPETokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = CharBPETokenizer(lowercase=False)\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer.train(\n",
    "    files=[corpus_file],\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<unk>\", \"<pad>\", \"</s>\"]\n",
    ")\n",
    "\n",
    "# Save vocab + merges\n",
    "tokenizer.save_model(\"/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer_bpe_char\")\n",
    "\n",
    "print(\"üíæ vocab.json and merges.txt written to ./tokenizer_bpe/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c84022",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges_txt = \"data/tokenizer_bpe/merges.txt\"\n",
    "\n",
    "\n",
    "with open(merges_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "    merges = [line.strip() for line in f if line and not line.startswith(\"#\")]\n",
    "\n",
    "print(\"üîé Total merge rules learned:\", len(merges))\n",
    "\n",
    "# Show first 50 merges (the most frequent pairs in your corpus)\n",
    "print(\"\\nTop 50 merge rules:\\n\")\n",
    "for i, merge in enumerate(merges[:50], 1):\n",
    "    print(f\"{i:02d}. {merge}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05cbaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import CharBPETokenizer\n",
    "\n",
    "# Load from vocab + merges\n",
    "tokenizer = CharBPETokenizer(\n",
    "    \"data/tokenizer_bpe_char/vocab.json\",\n",
    "    \"data/tokenizer_bpe_char/merges.txt\"\n",
    ")\n",
    "\n",
    "# Test it\n",
    "output = tokenizer.encode(\"‡Ωñ‡Ωë‡Ω∫‡ºã‡Ω£‡Ω∫‡ΩÇ‡Ω¶‡ºã\")\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"IDs:\", output.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import CharBPETokenizer\n",
    "\n",
    "# Load from vocab + merges\n",
    "loaded_tokenizer = CharBPETokenizer(\n",
    "    \"data/tokenizer_bpe_char/vocab.json\",\n",
    "    \"data/tokenizer_bpe_char/merges.txt\"\n",
    ")\n",
    "# Test encoding a sentence\n",
    "sentence = \"‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω¢‡æ£‡Ωò‡ºã‡Ωî‡ºã‡ΩÇ‡Ωâ‡Ω≤‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ωì‡ºã‡Ω°‡Ω∫‡ºã‡Ω§‡Ω∫‡Ω¶‡ºã‡Ω£‡æ∑‡ºã‡Ω£‡ºã‡Ω†‡ΩÇ‡æ±‡Ω¥‡Ω¢‡ºã‡Ω†‡ΩÇ‡æ≤‡Ωº‡ºã‡ΩÇ‡Ω≤‡ºã‡Ω°‡Ωº‡Ωë‡ºã‡Ω¢‡Ω∫‡Ωë‡ºç ‡Ωñ‡æ±‡ΩÑ‡ºã‡ΩÜ‡Ω¥‡Ωñ‡ºã‡ΩÄ‡æ±‡Ω≤‡ºã‡Ω¶‡Ω∫‡Ωò‡Ω¶‡ºã‡Ωò‡Ω∫‡Ωë‡ºã‡Ωì‡ºç ‡Ωë‡Ω∫‡ºã‡Ωì‡Ω¶‡ºã‡Ω†‡Ωë‡Ω≤‡ºã\"\n",
    "output = loaded_tokenizer.encode(sentence)\n",
    "\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(f\"Encoded tokens (IDs): {output.ids}\")\n",
    "print(f\"Encoded tokens (strings): {output.tokens}\")\n",
    "\n",
    "# The BPE algorithm learned to merge \"low\" + \"er\" -> \"lower\" and \"new\" + \"er\" -> \"newer\"\n",
    "sentence_2 = \"‡Ωñ‡æ≥‡ºã‡Ωò‡ºã‡Ωë‡ΩÇ‡Ω∫‡ºã‡Ωñ‡Ω†‡Ω≤‡ºã‡Ωñ‡Ω§‡Ω∫‡Ω¶‡ºã‡ΩÇ‡Ωâ‡Ω∫‡Ωì‡ºã‡Ω¶‡æê‡Ω¥‡ºã‡Ωë‡ΩÇ‡Ω∫‡ºã‡Ω†‡Ωë‡Ω¥‡Ωì‡ºã‡ΩÇ‡ΩÖ‡Ω≤‡ΩÇ‡ºã‡Ω°‡Ω≤‡Ωì‡ºã‡Ωì‡ºç\"\n",
    "output_2 = loaded_tokenizer.encode(sentence_2)\n",
    "print(f\"\\nOriginal sentence: {sentence_2}\")\n",
    "print(f\"Encoded tokens (IDs): {output_2.ids}\")\n",
    "print(f\"Encoded tokens (strings): {output_2.tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2e7727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training tokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=['/home/gangagyatso/Desktop/stt-bpe-trainer/data/corpus/mergedcorpus.txt'],\n",
    "                    min_frequency=2,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14a1824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"./data/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a759ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved vocab.json to /home/gangagyatso/Desktop/stt-bpe-trainer/data//vocab.json\n",
      "‚úÖ Saved merges.txt to /home/gangagyatso/Desktop/stt-bpe-trainer/data//merges.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Path to the saved tokenizer.json (from Script 1)\n",
    "tokenizer_path = \"/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer.json\"\n",
    "output_dir = \"/home/gangagyatso/Desktop/stt-bpe-trainer/data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# Extract vocab (token ‚Üí id mapping)\n",
    "vocab = tokenizer.get_vocab()\n",
    "# sort by id (important!)\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
    "\n",
    "with open(os.path.join(output_dir, \"vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({token: idx for token, idx in sorted_vocab}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved vocab.json to {output_dir}/vocab.json\")\n",
    "\n",
    "# Extract merges\n",
    "model = tokenizer.to_str()  # stringified JSON of the whole tokenizer\n",
    "model_json = json.loads(model)\n",
    "\n",
    "if \"model\" in model_json and \"merges\" in model_json[\"model\"]:\n",
    "    merges = model_json[\"model\"][\"merges\"]\n",
    "    with open(os.path.join(output_dir, \"merges.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"#version: 0.2\\n\")\n",
    "        for merge in merges:\n",
    "            f.write(\" \".join(merge) + \"\\n\")\n",
    "    print(f\"‚úÖ Saved merges.txt to {output_dir}/merges.txt\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No merges found in tokenizer.json (check if it's really BPE)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64105280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gangagyatso/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading tokenizer\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "old_tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Tibetan\", task=\"transcribe\")\n",
    "tokenizer = WhisperTokenizer(vocab_file='/home/gangagyatso/Desktop/stt-bpe-trainer/data/vocab.json',\n",
    "                            merges_file='/home/gangagyatso/Desktop/stt-bpe-trainer/data/merges.txt',\n",
    "                             unk_token='',\n",
    "                             bos_token= '<|endoftext|>',\n",
    "                             pad_token= '<|endoftext|>',\n",
    "                             model_max_length = 1024,\n",
    "                            language='Tibetan', task='transcribe')\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"bo\", task=\"transcribe\")\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': old_tokenizer.special_tokens_map['additional_special_tokens']\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b19cda2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer/tokenizer_config.json',\n",
       " '/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer/special_tokens_map.json',\n",
       " '/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer/vocab.json',\n",
       " '/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer/merges.txt',\n",
       " '/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer/normalizer.json',\n",
       " '/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"/home/gangagyatso/Desktop/stt-bpe-trainer/data/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec108070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
